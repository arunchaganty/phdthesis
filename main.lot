\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Examples highlighting the different modes in which the automatic metric and human judgments may agree or disagree. On the MS MARCO task, a majority of responses from systems were actually correct but poorly scored according to ROUGE-L. On the CNN/Daily Mail task, a significant number of examples which are scored highly by VecSim are poorly rated by humans, and likewise many examples scored poorly by VecSim are highly rated by humans. \relax }}{3}{table.caption.23}
\contentsline {table}{\numberline {4.2}{\ignorespaces A summary of the key statistics, human metric variance ($\sigma ^2_f$) and annotator variance ($\sigma ^2_a$) for different datasets, CNN/Daily Mail (CDM) and MS MARCO in our evaluation benchmark. We observe that the relative variance ($\gamma $) is fairly high for most evaluation prompts, upper bounding the data efficiency on these tasks. A notable exception is the \texttt {Edit} prompt wherein systems are compared on the number of post-edits required to improve their quality. \relax }}{11}{table.caption.27}
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces Datasets used in this paper and number of examples we evaluate on.\relax }}{26}{table.caption.41}
\contentsline {table}{\numberline {5.2}{\ignorespaces Results on NER and Face tasks comparing latencies, queries per token (Qs/tok) and performance metrics (\ensuremath {F_1}{} for NER and accuracy for Face).\relax }}{27}{table.caption.43}
\contentsline {table}{\numberline {5.3}{\ignorespaces Results on the Sentiment task comparing latency, queries per example and accuracy.\relax }}{28}{table.caption.44}
\addvspace {10\p@ }
