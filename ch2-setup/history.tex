\section{\label{sec:setup:history}Evaluation in NLP:\@ a brief history}

Defining an evaluation methodology provides a necessary framework for systematic progress, but can also over-simplify what it means to solve a problem: for example, while perplexity has long served as an evaluation metric for language modeling, it most certainly does not capture what it means to truly understand language.
Unsurprisingly, this has led to an ongoing debate in the field of artificial intelligence on how to balance philosophical ideals, like genuine language understanding, with pragmatism, like having a single quantitative indicator.
In this section, we revisit some of these discussions within the natural language processing field to provide some context for how the future of evaluation should look like.

\subsection{Early ideas}
Key papers dotting NLP evaluation thought.

PTB, language modeling.


\subsection{Shared tasks and test collections}
A history of tasks.
ACE, DUC, KBP

summarization tasks.



ROUGE and other metrics.


Newsblaster
Even with these simple systems, \citet{mckeown2005summaries} showed that providing users such multi-document summaries help.

Only recently have abstractive systems based on.
Multi-document summarization SUMMONS, NewsBlaster

Topic driven summarization

Information extraction based summarization.


Macro-rule systems.

