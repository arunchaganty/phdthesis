\section{\label{sec:setup:history}Evaluation in NLP:\@ a brief history}

Defining an evaluation methodology provides a necessary framework for systematic progress, but can also over-simplify what it means to solve a problem: for example, while perplexity has long served as an evaluation metric for language modeling, it also does not capture the entirety of what it means to truly understand language.
Unsurprisingly, this has led to an ongoing debate in the field of artificial intelligence on how to balance philosophical ideals, like genuine language understanding, with pragmatism, like having a single quantitative indicator.
In this section, we revisit some of these discussions within the natural language processing field to provide some context for how the future of evaluation should look like.

\subsection{Early ideas.}

Even as far back as 1956, before the development of many automated natural language processing systems, the future evaluation of these techniques was discussed.
\citet{miller1956psychological} is perhaps one of the first such papers to explore what modes of evaluation exist for machine translation, beyond asking humans for subjective input.
On the automated metric front, the paper introduces lexical overlap based scores that still underpin the most popular evaluation metrics today.
The authors recognized that the automatic metric were effective ``at the lower end of the scale'', but were unsure if it would remain differentiating between better systems.
As a result, they also looked at including humans in more objective evaluations, such as computing information gain by asking people to perform the Shannon test after reading the system generated translation, but concluded this might be far too laborious.
Finally, the authors also proposed using reading comprehension tests as an objective evaluation of whether the summary was able to correctly convey information: an exciting idea that is seeing recent interest, but with the challenge that picking the right questions can also be quite hard.

\subsection{An era of shared tasks.}

Fast forwarding through 40 years, a number of high-profile shared tasks were set up by the National Institute for Standards and Technology (NIST) in the 1990s for a variety of natural language understanding tasks through the TIPSTER program.
For example, the Text REtrieval Conference (TREC) was set up in 1992 to evaluate information retrieval systems and the Message Understanding Conference (MUC) was set up in the same period to evaluate information extraction systems.
Later, the SUMMAC extended the set of tasks to summarization.

Early on, a central question was how to evaluate these systems.
While the MUC tasks had more objective evaluation criteria like precision and recall, there has been a long debate about evaluation TREC.\@
\citet{webber2010measurement} provides an excellent summary of this work.
Perhaps the most interesting line of work on evaluation was in the TIPSTER summarization program~\citep{mani1999tipster}:
  systems were compared on an end-task based metric of how much they could speed up decision-making for analysts.
Later iterations of the summarization task in the Document Understanding Conference (DUC) used more general quality metrics like the quality and coverage of the generated summaries.

At the same time, the machine translation community also developed its own evaluation methodologies through the APRA MT program~\citep{white1994arpa}.
Initially, systems were compared by asking people to complete a multiple choice reading comprehension task using human generated and system generated translations.
Additionally, human panels were used to rate the adequacy and fluency of generated translations.

Unfortunately, the extensive human evaluations made it laborious to compare systems, particularly during development.
This motivated the development of automatic metrics, starting with BLEU~\citep{papineni02bleu}.
The automatic metric was proposed as a way to evaluate systems in the aggregate, with the hope that individual errors made by using a simple lexical overlap would be washed out as many different summaries were combined.
In the summarization community, ROUGE~\citep{lin2004rouge} was proposed as an efficient way to compute \textit{coverage} of the generated summary.
When first reported, it was observed that both the BLEU and ROUGE metrics had had high correlation with human ratings when measured at the system level.
Unfortunately, over time, it was found that these metrics were inadequate when systems began use a more diverse vocabulary~\citep{lavie2009meteor,cohan2016revisiting}

% 2. adding linguistic information with CiDER, METEOR, etc.
This prompted development of variants that used additional linguistic information or word frequencies, e.g. METEOR~\citep{lavie2009meteor,denkowski2014meteor} or CiDER~\citep{vedantam2015cider}.
% russo-lassner 2005
% TODO: find grammar-based metrics to cite
Despite these developments, \citet{liu2016evaluate} and \citet{novikova2017why} find that a whole suite of automatic evaluation metrics---including those mentioned above---correlate incredibly poorly with human judgment across different datasets and systems, with Pearson's $\rho$ between $0$ and $0.3$.

% ==NOTE(chaganty): Too much detail, not enough space.
%% 4. another approach has been use use multiple references
Another approach has been to use multiple references, which tends to improve correlation with humans.
Indeed, \citet{toutanova2016dataset} report that multiple reference variants of metrics significantly score higher than others, though this can also be low.
Unfortunately, there is no clear understanding on how many references are sufficient: 
  \citet{culy2003limits} report needing atleast 4 reference translations to effectively employ BLEU to a given correlation.
  \citet{lavie2009meteor} find that more references helped correlation for METEOR but only marginally.
\citet{vanhalteren2003factoid} required up to 40--50 reference factoid summaries for stable consensus.
%\ac{I'm probably going to omit the above for lack of space.}

% 3. include learning-based approaches for metrics
Finally, the community has also tried to tuning evaluation metrics to optimize for human correlation~\citep{lavie2009meteor,denkowski2014meteor,lowe2017towards}.
While these approaches do increase the correlation for systems in the tune set, their correlations with new systems can be substantially smaller: \citet{lowe2017towards}, for example, find that correlations for held-out systems have an average Pearson correlation of $0.13$, almost a third of the average correlation of systems that were part of the tune set ($0.37$).

