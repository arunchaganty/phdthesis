\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{Preface}{iv}{chapter*.1}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{v}{chapter*.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Setup}{2}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{berant2013freebase,fader2014open,reddy2014large}
\citation{kalyanpur2012structured}
\citation{han2015exploiting}
\citation{sparck1975report,harman1993trec}
\citation{zobel1998reliable}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Importance sampling for unbiased on-demand evaluation of knowledge base population}{3}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{3}{section.3.1}}
\newlabel{sec:intro}{{3.1}{3}{Introduction}{section.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  An example describing entities and relations in knowledge base population.\relax }}{4}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{3.1}{4}{An example describing entities and relations in knowledge base population.\relax }{figure.caption.6}{}}
\citation{ji2011kbp}
\citation{ellis2012kbp}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Background}{5}{section.3.2}}
\newlabel{sec:setup}{{3.2}{5}{Background}{section.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Pooled evaluation.}{5}{section*.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  In pooled evaluation, an evaluation dataset is constructed by labeling relation instances collected from the pooled systems (A and B) and from a team of human annotators (Humans). However, when a new system (C) is evaluated on this dataset, some of its predictions ($i_6$) are missing and can not be fairly evaluated. Here, the precision and recall for C should be $\frac  {3}{3}$ and $\frac  {3}{4}$ respectively, but its evaluation scores are estimated to be $\frac  {2}{3}$ and $\frac  {2}{3}$. The discrepancy between these two scores is called \textit  {pooling bias}. \relax }}{6}{figure.caption.7}}
\newlabel{fig:pooling}{{3.2}{6}{In pooled evaluation, an evaluation dataset is constructed by labeling relation instances collected from the pooled systems (A and B) and from a team of human annotators (Humans). However, when a new system (C) is evaluated on this dataset, some of its predictions ($i_6$) are missing and can not be fairly evaluated. Here, the precision and recall for C should be $\frac {3}{3}$ and $\frac {3}{4}$ respectively, but its evaluation scores are estimated to be $\frac {2}{3}$ and $\frac {2}{3}$. The discrepancy between these two scores is called \textit {pooling bias}. \relax }{figure.caption.7}{}}
\citation{zobel1998reliable}
\citation{dang2016kbp}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Measuring pooling bias}{7}{section.3.3}}
\newlabel{sec:analysis}{{3.3}{7}{Measuring pooling bias}{section.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Measuring bias.}{7}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{Results.}{7}{section*.11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}On-demand evaluation with importance sampling}{7}{section.3.4}}
\newlabel{sec:method}{{3.4}{7}{On-demand evaluation with importance sampling}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Problem statement}{8}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Simple estimators}{8}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Joint estimators}{8}{subsection.3.4.3}}
\newlabel{sec:joint}{{3.4.3}{8}{Joint estimators}{subsection.3.4.3}{}}
\citation{owen2013monte}
\@writefile{toc}{\contentsline {paragraph}{Estimating precision jointly.}{9}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{Estimating recall jointly.}{9}{section*.13}}
\citation{burden1985bisection}
\citation{webber2010measurement}
\@writefile{toc}{\contentsline {paragraph}{Adaptively choosing the number of samples.}{10}{section*.14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}On-demand evaluation for KBP}{10}{section.3.5}}
\newlabel{sec:application}{{3.5}{10}{On-demand evaluation for KBP}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Sampling from system predictions}{10}{subsection.3.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Labeling predicted instances}{11}{subsection.3.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Sampling true instances}{11}{subsection.3.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Evaluation}{12}{section.3.6}}
\newlabel{sec:evaluation}{{3.6}{12}{Evaluation}{section.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Bias and variance of the on-demand evaluation.}{12}{subsection.3.6.1}}
\citation{manning2014stanford}
\citation{ratinov2011local}
\citation{zobel1998reliable}
\citation{buckley2007bias}
\citation{webber2010measurement}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Number of samples required by on-demand evaluation}{13}{subsection.3.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}A mock evaluation for TAC KBP 2016}{13}{subsection.3.6.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Related work}{13}{section.3.7}}
\newlabel{sec:related}{{3.7}{13}{Related work}{section.3.7}{}}
\citation{zobel1998reliable,cormack1998efficient,aslam2006statistical}
\citation{buckley2004incomplete,sakai2008information,aslam2006statistical}
\citation{aslam2006statistical}
\citation{yilmaz2008simple}
\citation{vannella2014validating,angeli2014combining,he2015question,liu2016effective}
\citation{pavlick2016gun}
\citation{angeli2014combining,adel2016comparing}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Discussion}{14}{section.3.8}}
\newlabel{sec:discussion}{{3.8}{14}{Discussion}{section.3.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  Median pooling bias (difference between pooled and unpooled scores) on the top 40 systems of TAC KBP 2015 evaluation using the official and \texttt  {anydoc} {} scores. The bias is much smaller for the lenient \texttt  {anydoc} {} metric, but even so, it is larger than the largest difference between adjacent systems (1.5\% \ensuremath  {F_1}{}) and typical system improvements (around 1\% \ensuremath  {F_1}{}). \relax }}{15}{figure.caption.9}}
\newlabel{fig:pooling-bias}{{3.3}{15}{Median pooling bias (difference between pooled and unpooled scores) on the top 40 systems of TAC KBP 2015 evaluation using the official and \anydoc {} scores. The bias is much smaller for the lenient \anydoc {} metric, but even so, it is larger than the largest difference between adjacent systems (1.5\% \fone {}) and typical system improvements (around 1\% \fone {}). \relax }{figure.caption.9}{}}
\newlabel{fig:relation-interface}{{3.4a}{16}{\relax }{figure.caption.15}{}}
\newlabel{sub@fig:relation-interface}{{a}{16}{\relax }{figure.caption.15}{}}
\newlabel{fig:entity-interface}{{3.4b}{16}{\relax }{figure.caption.15}{}}
\newlabel{sub@fig:entity-interface}{{b}{16}{\relax }{figure.caption.15}{}}
\newlabel{fig:evaluation-results}{{3.4f}{16}{\relax }{figure.caption.15}{}}
\newlabel{sub@fig:evaluation-results}{{f}{16}{\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  \textbf  {(a, b):} Interfaces for annotating relations and entities respectively. \textbf  {(c, d):} A comparison of bias for the pooling, simple and joint estimators on the TAC KBP 2015 challenge. Each point in the figure is a mean of 500 repeated trials; dotted lines show the 90\% quartile. Both the simple and joint estimators are unbiased, and the joint estimator is able to significantly reduce variance. \textbf  {(e):} A comparison of the number of samples used to estimate scores under the fixed and adaptive sample selection scheme. Each faint line shows the number of samples used during a single trial, while solid lines show the mean over 100 trials. The dashed line shows a square-root relationship between the number of systems evaluated and the number of samples required. Thus joint estimation combined with adaptive sample selection can reduce the number of labeled annotations required by an order of magnitude. \textbf  {(f):} Precision ($P$), recall ($R$) and \ensuremath  {F_1}{} scores from a pilot run of our evaluation service for ensembles of a rule-based system (R), a logistic classifier (L) and a neural network classifier (N) run on the TAC KBP 2016 document corpus. \relax }}{16}{figure.caption.15}}
\newlabel{fig:simulation}{{3.4}{16}{\textbf {(a, b):} Interfaces for annotating relations and entities respectively. \textbf {(c, d):} A comparison of bias for the pooling, simple and joint estimators on the TAC KBP 2015 challenge. Each point in the figure is a mean of 500 repeated trials; dotted lines show the 90\% quartile. Both the simple and joint estimators are unbiased, and the joint estimator is able to significantly reduce variance. \textbf {(e):} A comparison of the number of samples used to estimate scores under the fixed and adaptive sample selection scheme. Each faint line shows the number of samples used during a single trial, while solid lines show the mean over 100 trials. The dashed line shows a square-root relationship between the number of systems evaluated and the number of samples required. Thus joint estimation combined with adaptive sample selection can reduce the number of labeled annotations required by an order of magnitude. \textbf {(f):} Precision ($P$), recall ($R$) and \fone {} scores from a pilot run of our evaluation service for ensembles of a rule-based system (R), a logistic classifier (L) and a neural network classifier (N) run on the TAC KBP 2016 document corpus. \relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}The price of debiasing automatic metrics in natural language evaluation}{17}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{nallapati2016abstractive}
\citation{nguyen2016ms,kovcisky2017narrativeqa}
\citation{lin2014microsoft}
\citation{lowe2017ubuntu}
\citation{papineni02bleu}
\citation{lin2004rouge}
\citation{lavie2009meteor,denkowski2014meteor}
\citation{vedantam2015cider}
\citation{liu2016evaluate,novikova2017why}
\citation{ripley2009stochastic}
\newlabel{sec:intro}{{4.1}{1}{Introduction}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{1}{section.4.1}}
\newlabel{fig:bias-msmarco-system}{{4.1a}{1}{System-level correlation on the MS MARCO task\relax }{figure.caption.16}{}}
\newlabel{sub@fig:bias-msmarco-system}{{a}{1}{System-level correlation on the MS MARCO task\relax }{figure.caption.16}{}}
\newlabel{fig:bias-msmarco-instance}{{4.1b}{1}{Instance-level correlation for the \texttt {fastqa} system\relax }{figure.caption.16}{}}
\newlabel{sub@fig:bias-msmarco-instance}{{b}{1}{Instance-level correlation for the \texttt {fastqa} system\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  (a) At a system-level, automatic metrics (ROUGE-L) and human judgment correlate well, but (b) the instance-level correlation plot (where each point is a system prediction) shows that the instance-level correlation is quite low ($\rho = 0.31$). As a consequence, if we try to locally improve systems to produce better answers ($\triangleright $ in (a)), they do not significantly improve ROUGE scores and vice versa ($\vartriangle $). \relax }}{1}{figure.caption.16}}
\newlabel{fig:bias-msmarco}{{4.1}{1}{(a) At a system-level, automatic metrics (ROUGE-L) and human judgment correlate well, but (b) the instance-level correlation plot (where each point is a system prediction) shows that the instance-level correlation is quite low ($\rho = 0.31$). As a consequence, if we try to locally improve systems to produce better answers ($\triangleright $ in (a)), they do not significantly improve ROUGE scores and vice versa ($\vartriangle $). \relax }{figure.caption.16}{}}
\citation{hermann2015read,nallapati2016abstractive}
\citation{nguyen2016ms}
\citation{novikova2017why}
\citation{liu2016evaluate}
\citation{novikova2017why}
\citation{nguyen2016ms}
\citation{novikova2017why}
\citation{conroy2008mind}
\citation{dang2006overview}
\citation{wu2016google}
\citation{chaganty2017unbiased}
\newlabel{sec:bias}{{4.2}{2}{Bias in automatic evaluation}{section.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2} Bias in automatic evaluation}{2}{section.4.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces  Examples highlighting the different modes in which the automatic metric and human judgments may agree or disagree. On the MS MARCO task, a majority of responses from systems were actually correct but poorly scored according to ROUGE-L. On the CNN/Daily Mail task, a significant number of examples which are scored highly by VecSim are poorly rated by humans, and likewise many examples scored poorly by VecSim are highly rated by humans. \relax }}{3}{table.caption.17}}
\newlabel{tab:examples}{{4.1}{3}{Examples highlighting the different modes in which the automatic metric and human judgments may agree or disagree. On the MS MARCO task, a majority of responses from systems were actually correct but poorly scored according to ROUGE-L. On the CNN/Daily Mail task, a significant number of examples which are scored highly by VecSim are poorly rated by humans, and likewise many examples scored poorly by VecSim are highly rated by humans. \relax }{table.caption.17}{}}
\citation{ripley2009stochastic}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Statistical estimation for unbiased evaluation}{4}{section.4.3}}
\newlabel{sec:method}{{4.3}{4}{Statistical estimation for unbiased evaluation}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Sample mean}{4}{subsection.4.3.1}}
\newlabel{eqn:varsimple}{{4.2}{4}{Sample mean}{equation.4.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Control variates estimator}{5}{subsection.4.3.2}}
\newlabel{thm:main}{{1}{5}{}{theorem.1}{}}
\newlabel{eqn:varcontrol}{{4.6}{5}{}{equation.4.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces  The samples from $f(z)$ have a higher variance than the samples from $f(z)-g(z)$ but the same mean. This is the key idea behind using control variates to reduce variance.\relax }}{6}{figure.caption.18}}
\newlabel{fig:variance_reduction}{{4.2}{6}{The samples from $f(z)$ have a higher variance than the samples from $f(z)-g(z)$ but the same mean. This is the key idea behind using control variates to reduce variance.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Using the control variates estimator}{6}{subsection.4.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  Inverse data efficiency for various values of $\gamma $ and $\rho $. We need both low $\gamma $ and high $\rho $ to obtain significant gains. \relax }}{7}{figure.caption.19}}
\newlabel{fig:savings}{{4.3}{7}{Inverse data efficiency for various values of $\gamma $ and $\rho $. We need both low $\gamma $ and high $\rho $ to obtain significant gains. \relax }{figure.caption.19}{}}
\citation{mnih2008empirical}
\citation{passonneau2014benefits}
\citation{hermann2015read,nallapati2016abstractive}
\newlabel{prop:added_bias}{{1}{8}{}{proposition.1}{}}
\newlabel{alg:estimate}{{1}{8}{Control variates estimator\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Control variates estimator\relax }}{8}{algorithm.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Discussion of assumptions}{8}{subsection.4.3.4}}
\newlabel{sec:tasks}{{4.4}{8}{Tasks and datasets}{section.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4} Tasks and datasets}{8}{section.4.4}}
\@writefile{toc}{\contentsline {paragraph}{Evaluating language quality in automatic summarization.}{8}{section*.22}}
\citation{dang2006overview}
\citation{snover2006ter}
\citation{see2017point}
\citation{paulus2018deep}
\citation{manning2014stanford}
\citation{nguyen2016ms}
\newlabel{fig:interfaces-edit}{{4.4a}{9}{Interface to evaluate language quality on CNN/Daily Mail\relax }{figure.caption.20}{}}
\newlabel{sub@fig:interfaces-edit}{{a}{9}{Interface to evaluate language quality on CNN/Daily Mail\relax }{figure.caption.20}{}}
\newlabel{fig:interfaces-qa}{{4.4b}{9}{Interface to judge answer correctness on MS MARCO\relax }{figure.caption.20}{}}
\newlabel{sub@fig:interfaces-qa}{{b}{9}{Interface to judge answer correctness on MS MARCO\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces  Screenshots of the annotation interfaces we used to measure (a) summary language quality on CNN/Daily Mail and (b) answer correctness on MS MARCO tasks. \relax }}{9}{figure.caption.20}}
\newlabel{fig:tasks}{{4.4}{9}{Screenshots of the annotation interfaces we used to measure (a) summary language quality on CNN/Daily Mail and (b) answer correctness on MS MARCO tasks. \relax }{figure.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces  A summary of the key statistics, human metric variance ($\sigma ^2_f$) and annotator variance ($\sigma ^2_a$) for different datasets, CNN/Daily Mail (CDM) and MS MARCO in our evaluation benchmark. We observe that the relative variance ($\gamma $) is fairly high for most evaluation prompts, upper bounding the data efficiency on these tasks. A notable exception is the \texttt  {Edit} prompt wherein systems are compared on the number of post-edits required to improve their quality. \relax }}{9}{table.caption.21}}
\newlabel{tab:dataset}{{4.2}{9}{A summary of the key statistics, human metric variance ($\sigma ^2_f$) and annotator variance ($\sigma ^2_a$) for different datasets, CNN/Daily Mail (CDM) and MS MARCO in our evaluation benchmark. We observe that the relative variance ($\gamma $) is fairly high for most evaluation prompts, upper bounding the data efficiency on these tasks. A notable exception is the \texttt {Edit} prompt wherein systems are compared on the number of post-edits required to improve their quality. \relax }{table.caption.21}{}}
\citation{weissenborn2017fastqa}
\citation{tan2018s}
\@writefile{toc}{\contentsline {paragraph}{Evaluating answer correctness.}{10}{section*.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  Correlations of different automatic metrics on the MS MARCO and CNN/Daily Mail tasks. Certain systems are more correlated with certain automatic metrics than others, but overall the correlation is low to moderate for most systems and metrics. \relax }}{10}{figure.caption.24}}
\newlabel{fig:correlation}{{4.5}{10}{Correlations of different automatic metrics on the MS MARCO and CNN/Daily Mail tasks. Certain systems are more correlated with certain automatic metrics than others, but overall the correlation is low to moderate for most systems and metrics. \relax }{figure.caption.24}{}}
\citation{papineni02bleu}
\citation{lin2004rouge}
\citation{lavie2009meteor}
\citation{novikova2017why}
\citation{liu2016evaluate}
\citation{pagliardini2017unsupervised}
\newlabel{sec:evaluation}{{4.5}{11}{Experimental results}{section.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Experimental results}{11}{section.4.5}}
\@writefile{toc}{\contentsline {paragraph}{Automatic metrics.}{11}{section*.25}}
\newlabel{fig:trajectory-a}{{4.6a}{11}{\texttt {seq2seq} on CNN/Daily Mail using the \texttt {Overall}\relax }{figure.caption.26}{}}
\newlabel{sub@fig:trajectory-a}{{a}{11}{\texttt {seq2seq} on CNN/Daily Mail using the \texttt {Overall}\relax }{figure.caption.26}{}}
\newlabel{fig:trajectory-b}{{4.6b}{11}{\texttt {seq2seq} on CNN/Daily Mail using \texttt {Edit} \relax }{figure.caption.26}{}}
\newlabel{sub@fig:trajectory-b}{{b}{11}{\texttt {seq2seq} on CNN/Daily Mail using \texttt {Edit} \relax }{figure.caption.26}{}}
\newlabel{fig:trajectory-c}{{4.6c}{11}{\texttt {fastqa\_ext} on MS MARCO using \texttt {AnyCorrect}\relax }{figure.caption.26}{}}
\newlabel{sub@fig:trajectory-c}{{c}{11}{\texttt {fastqa\_ext} on MS MARCO using \texttt {AnyCorrect}\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces  80\% bootstrap confidence interval length as a function of the number of human judgments used when evaluating the indicated systems on their respective datasets and prompts. (a) We see a modest reduction in variance (and hence cost) relative to human evaluation by using the VecSim automatic metric with the proposed control variates estimator to estimate \texttt  {Overall} scores on the CNN/Daily Mail task; the data efficiency (DE) is $1.06$. (b) By improving the evaluation prompt to use \texttt  {Edit}s instead, it is possible to further reduce variance relative to humans (DE is $1.15$). (c) Another way to reduce variance relative to humans is to improve the automatic metric evaluation; here using ROUGE-1 instead of VecSim improves the DE from $1.03$ to $1.16$. \relax }}{11}{figure.caption.26}}
\newlabel{fig:trajectory}{{4.6}{11}{80\% bootstrap confidence interval length as a function of the number of human judgments used when evaluating the indicated systems on their respective datasets and prompts. (a) We see a modest reduction in variance (and hence cost) relative to human evaluation by using the VecSim automatic metric with the proposed control variates estimator to estimate \texttt {Overall} scores on the CNN/Daily Mail task; the data efficiency (DE) is $1.06$. (b) By improving the evaluation prompt to use \texttt {Edit}s instead, it is possible to further reduce variance relative to humans (DE is $1.15$). (c) Another way to reduce variance relative to humans is to improve the automatic metric evaluation; here using ROUGE-1 instead of VecSim improves the DE from $1.03$ to $1.16$. \relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {paragraph}{Results.}{11}{section*.27}}
\citation{lowe2017towards,dusek2017referenceless}
\citation{ripley2009stochastic}
\citation{greensmith2004variance,paisley2012variational,ranganath2014black}
\citation{chaganty2017unbiased}
\citation{chang2017affordable}
\newlabel{sec:setup}{{4.6}{12}{Related work}{section.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6} Related work}{12}{section.4.6}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Discussion}{13}{section.4.7}}
\newlabel{sec:discussion}{{4.7}{13}{Discussion}{section.4.7}{}}
\citation{deng2009imagenet}
\citation{krizhevsky2012imagenet}
\citation{bernstein2010soylent,kokkalis2013emailvalet}
\citation{li2012twiner}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}On-the-Job Learning with Bayesian Decision Theory}{14}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{14}{section.5.1}}
\newlabel{sec:intro}{{5.1}{14}{Introduction}{section.5.1}{}}
\citation{lasecki2013real}
\citation{cesabianchi06prediction}
\citation{helmbold1997some,sculley2007online,chu2011unbiased}
\citation{gao2011active}
\citation{kocsis2006bandit}
\citation{coulom2007computing}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Problem formulation}{15}{section.5.2}}
\newlabel{sec:model}{{5.2}{15}{Problem formulation}{section.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces  Named entity recognition on tweets in on-the-job learning. \relax }}{16}{figure.caption.28}}
\newlabel{fig:crf}{{5.1}{16}{Named entity recognition on tweets in on-the-job learning. \relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Model}{16}{section.5.3}}
\newlabel{sec:model}{{5.3}{16}{Model}{section.5.3}{}}
\newlabel{fig:behavior}{{5.2a}{17}{{\bf Incorporating information from responses.} The bar graphs represent the marginals over the labels for each token (indicated by the first character) at different points in time. The two timelines show how the system updates its confidence over labels based on the crowd's responses. The system continues to issue queries until it has sufficient confidence on its labels. See the paragraph on behavior in \refsec {model} for more information. \relax }{figure.caption.29}{}}
\newlabel{sub@fig:behavior}{{a}{17}{{\bf Incorporating information from responses.} The bar graphs represent the marginals over the labels for each token (indicated by the first character) at different points in time. The two timelines show how the system updates its confidence over labels based on the crowd's responses. The system continues to issue queries until it has sufficient confidence on its labels. See the paragraph on behavior in \refsec {model} for more information. \relax }{figure.caption.29}{}}
\newlabel{fig:tree}{{5.2b}{17}{{\bf Game tree.} An example of a partial game tree constructed by the system when deciding which action to take in the state $\sigma = (1, (3), (0), (\emptyset ), (\emptyset ))$, i.e.\ the query $q_1 = 3$ has already been issued and the system must decide whether to issue another query or wait for a response to $q_1$. \relax }{figure.caption.29}{}}
\newlabel{sub@fig:tree}{{b}{17}{{\bf Game tree.} An example of a partial game tree constructed by the system when deciding which action to take in the state $\sigma = (1, (3), (0), (\emptyset ), (\emptyset ))$, i.e.\ the query $q_1 = 3$ has already been issued and the system must decide whether to issue another query or wait for a response to $q_1$. \relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Example behavior while running structure prediction on the tweet ``Soup on George str.'' We omit the \textsc  {resource}{} from the game tree for visual clarity. \relax }}{17}{figure.caption.29}}
\newlabel{fig:game-tree}{{5.2}{17}{Example behavior while running structure prediction on the tweet ``Soup on George str.'' We omit the \scres {} from the game tree for visual clarity. \relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {paragraph}{Game tree.}{17}{section*.30}}
\@writefile{toc}{\contentsline {paragraph}{Utility.}{18}{section*.31}}
\newlabel{eqn:utility}{{5.1}{18}{Utility}{equation.5.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Environment model.}{18}{section*.32}}
\newlabel{eqn:dynamics}{{5.3}{18}{Environment model}{equation.5.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Behavior.}{18}{section*.33}}
\citation{kocsis2006bandit}
\citation{coulom2007computing}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Game playing}{19}{section.5.4}}
\newlabel{sec:game-playing}{{5.4}{19}{Game playing}{section.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Experiments}{19}{section.5.5}}
\newlabel{sec:experiments}{{5.5}{19}{Experiments}{section.5.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Baselines.}{19}{section*.34}}
\citation{finkel2005incorporating}
\citation{maas2011learning}
\citation{socher2013recursive}
\citation{attribute_classifiers}
\citation{krizhevsky2012imagenet}
\citation{bernstein2011crowds}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Approximating expected utility with MCTS and progressive widening\relax }}{20}{algorithm.2}}
\newlabel{algo:mcvts}{{2}{20}{Approximating expected utility with MCTS and progressive widening\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation and crowdsourcing setup.}{20}{section*.36}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Datasets used in this paper and number of examples we evaluate on.\relax }}{21}{table.caption.35}}
\newlabel{tbl:dataset}{{5.1}{21}{Datasets used in this paper and number of examples we evaluate on.\relax }{table.caption.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Results on NER and Face tasks comparing latencies, queries per token (Qs/tok) and performance metrics (\ensuremath  {F_1}{} for NER and accuracy for Face).\relax }}{21}{table.caption.37}}
\newlabel{tbl:results}{{5.2}{21}{Results on NER and Face tasks comparing latencies, queries per token (Qs/tok) and performance metrics (\fone {} for NER and accuracy for Face).\relax }{table.caption.37}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary of results.}{21}{section*.39}}
\citation{cesabianchi06prediction}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Queries per example for LENSE on Sentiment. With simple \textsc  {unigram} features, the model quickly learns it does not have the capacity to answer confidently and must query the crowd. With more complex \textsc  {rnn} features, the model learns to be more confident and queries the crowd less over time.\relax }}{22}{table.caption.38}}
\newlabel{fig:sentiment-tradeoff}{{5.3}{22}{Queries per example for LENSE on Sentiment. With simple \textsc {unigram} features, the model quickly learns it does not have the capacity to answer confidently and must query the crowd. With more complex \textsc {rnn} features, the model learns to be more confident and queries the crowd less over time.\relax }{table.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Results on the Sentiment task comparing latency, queries per example and accuracy.\relax }}{22}{table.caption.38}}
\newlabel{tbl:sentiment-results}{{5.3}{22}{Results on the Sentiment task comparing latency, queries per example and accuracy.\relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {paragraph}{Reproducibility.}{22}{section*.41}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Related Work}{22}{section.5.6}}
\newlabel{sec:related}{{5.6}{22}{Related Work}{section.5.6}{}}
\citation{settles2010active}
\citation{helmbold1997some,sculley2007online,chu2011unbiased}
\citation{donmez2008proactive,golovin2010near}
\citation{greiner2002learning,chai2004test,esmeir2007anytime}
\citation{lasecki2013real}
\citation{cheng2015flock}
\citation{dai2010decision}
\citation{liang09measurements}
\citation{angeli2014combining}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparing \ensuremath  {F_1}{} and queries per token on the NER task over time. The left graph compares LENSE to online learning (which cannot query humans at test time). This highlights that LENSE maintains high \ensuremath  {F_1}{} scores even with very small training set sizes, by falling back the crowd when it is unsure. The right graph compares query rate over time to 1-vote. This clearly shows that as the model learns, it needs to query the crowd less.\relax }}{23}{figure.caption.40}}
\newlabel{fig:ner-f1}{{5.4}{23}{Comparing \fone {} and queries per token on the NER task over time. The left graph compares LENSE to online learning (which cannot query humans at test time). This highlights that LENSE maintains high \fone {} scores even with very small training set sizes, by falling back the crowd when it is unsure. The right graph compares query rate over time to 1-vote. This clearly shows that as the model learns, it needs to query the crowd less.\relax }{figure.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Conclusion}{24}{section.5.7}}
\newlabel{sec:conclusion}{{5.7}{24}{Conclusion}{section.5.7}{}}
\bibstyle{emnlp_natbib}
\bibdata{all}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions: where do we go from here?}{25}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{adel2016comparing}{{1}{2016}{{Adel et~al.}}{{Adel, Roth, and Sch\"{u}tze}}}
\bibcite{angeli2014combining}{{2}{2014}{{Angeli et~al.}}{{Angeli, Tibshirani, Wu, and Manning}}}
\bibcite{aslam2006statistical}{{3}{2006}{{Aslam et~al.}}{{Aslam, Pavlu, and Yilmaz}}}
\bibcite{berant2013freebase}{{4}{2013}{{Berant et~al.}}{{Berant, Chou, Frostig, and Liang}}}
\bibcite{bernstein2011crowds}{{5}{2011}{{Bernstein et~al.}}{{Bernstein, Brandt, Miller, and Karger}}}
\bibcite{bernstein2010soylent}{{6}{2010}{{Bernstein et~al.}}{{Bernstein, Little, Miller, Hartmann, Ackerman, Karger, Crowell, and Panovich}}}
\bibcite{buckley2007bias}{{7}{2007}{{Buckley et~al.}}{{Buckley, Dimmick, Soboroff, and Voorhees}}}
\bibcite{buckley2004incomplete}{{8}{2004}{{Buckley and Voorhees}}{{}}}
\bibcite{burden1985bisection}{{9}{1985}{{Burden and Faires}}{{}}}
\bibcite{cesabianchi06prediction}{{10}{2006}{{Cesa-Bianchi and Lugosi}}{{}}}
\bibcite{chaganty2017unbiased}{{11}{2017}{{Chaganty et~al.}}{{Chaganty, Paranjape, Liang, and Manning}}}
\bibcite{chai2004test}{{12}{2004}{{Chai et~al.}}{{Chai, Deng, Yang, and Ling}}}
\bibcite{chang2017affordable}{{13}{2017}{{Chang et~al.}}{{Chang, Yang, Chen, Zhou, and Yu}}}
\bibcite{cheng2015flock}{{14}{2015}{{Cheng and Bernstein}}{{}}}
\bibcite{chu2011unbiased}{{15}{2011}{{Chu et~al.}}{{Chu, Zinkevich, Li, Thomas, and Tseng}}}
\bibcite{conroy2008mind}{{16}{2008}{{Conroy and Dang}}{{}}}
\bibcite{cormack1998efficient}{{17}{1998}{{Cormack et~al.}}{{Cormack, Palmer, and Clarke}}}
\bibcite{coulom2007computing}{{18}{2007}{{Coulom}}{{}}}
\bibcite{dai2010decision}{{19}{2010}{{Dai et~al.}}{{Dai, Mausam, and Weld}}}
\bibcite{dang2006overview}{{20}{2006}{{Dang}}{{}}}
\bibcite{dang2016kbp}{{21}{2016}{{Dang}}{{}}}
\bibcite{deng2009imagenet}{{22}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{denkowski2014meteor}{{23}{2014}{{Denkowski and Lavie}}{{}}}
\bibcite{donmez2008proactive}{{24}{2008}{{Donmez and Carbonell}}{{}}}
\bibcite{dusek2017referenceless}{{25}{2017}{{Dusek et~al.}}{{Dusek, Novikova, and Rieser}}}
\bibcite{ellis2012kbp}{{26}{2012}{{Ellis et~al.}}{{Ellis, Li, Griffitt, and Strassel}}}
\bibcite{esmeir2007anytime}{{27}{2007}{{Esmeir and Markovitch}}{{}}}
\bibcite{fader2014open}{{28}{2014}{{Fader et~al.}}{{Fader, Zettlemoyer, and Etzioni}}}
\bibcite{finkel2005incorporating}{{29}{2005}{{Finkel et~al.}}{{Finkel, Grenager, and Manning}}}
\bibcite{gao2011active}{{30}{2011}{{Gao and Koller}}{{}}}
\bibcite{golovin2010near}{{31}{2010}{{Golovin et~al.}}{{Golovin, Krause, and Ray}}}
\bibcite{greensmith2004variance}{{32}{2004}{{Greensmith et~al.}}{{Greensmith, Bartlett, and Baxter}}}
\bibcite{greiner2002learning}{{33}{2002}{{Greiner et~al.}}{{Greiner, Grove, and Roth}}}
\bibcite{han2015exploiting}{{34}{2015}{{Han et~al.}}{{Han, Bang, Ryu, and Lee}}}
\bibcite{harman1993trec}{{35}{1993}{{Harman}}{{}}}
\bibcite{he2015question}{{36}{2015}{{He et~al.}}{{He, Lewis, and Zettlemoyer}}}
\bibcite{helmbold1997some}{{37}{1997}{{Helmbold and Panizza}}{{}}}
\bibcite{hermann2015read}{{38}{2015}{{Hermann et~al.}}{{Hermann, Ko\IeC {\v c}isk\IeC {\'y}, Grefenstette, Espeholt, Kay, Suleyman, and Blunsom}}}
\bibcite{ji2011kbp}{{39}{2011}{{Ji et~al.}}{{Ji, Grishman, and {Trang Dang}}}}
\bibcite{sparck1975report}{{40}{1975}{{Jones and Rijsbergen}}{{}}}
\bibcite{kalyanpur2012structured}{{41}{2012}{{Kalyanpur et~al.}}{{Kalyanpur, Boguraev, Patwardhan, Murdock, Lally, Welty, Prager, Coppola, Fokoue-Nkoutche, Zhang, Pan, and Qui}}}
\bibcite{kovcisky2017narrativeqa}{{42}{2017}{{Ko{\v {c}}isky et~al.}}{{Ko{\v {c}}isky, Schwarz, Blunsom, Dyer, Hermann, Melis, and Grefenstette}}}
\bibcite{kocsis2006bandit}{{43}{2006}{{Kocsis and Szepesv{\'a}ri}}{{}}}
\bibcite{kokkalis2013emailvalet}{{44}{2013}{{Kokkalis et~al.}}{{Kokkalis, K{\"o}hn, Pfeiffer, Chornyi, Bernstein, and Klemmer}}}
\bibcite{krizhevsky2012imagenet}{{45}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{lavie2009meteor}{{46}{2009}{{Lavie and Denkowski}}{{}}}
\bibcite{li2012twiner}{{47}{2012}{{Li et~al.}}{{Li, Weng, He, Yao, Datta, Sun, and Lee}}}
\bibcite{liang09measurements}{{48}{2009}{{Liang et~al.}}{{Liang, Jordan, and Klein}}}
\bibcite{lin2004rouge}{{49}{2004}{{Lin and Rey}}{{}}}
\bibcite{lin2014microsoft}{{50}{2014}{{Lin et~al.}}{{Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{'a}r, and Zitnick}}}
\bibcite{liu2016effective}{{51}{2016{a}}{{Liu et~al.}}{{Liu, Soderland, Bragg, Lin, Ling, and Weld}}}
\bibcite{liu2016evaluate}{{52}{2016{b}}{{Liu et~al.}}{{Liu, Lowe, Serban, Noseworthy, Charlin, and Pineau}}}
\bibcite{lowe2017towards}{{53}{2017{a}}{{Lowe et~al.}}{{Lowe, Noseworthy, Serban, Angelard-Gontier, Bengio, and Pineau}}}
\bibcite{lowe2017ubuntu}{{54}{2017{b}}{{Lowe et~al.}}{{Lowe, Pow, Serban, Charlin, Liu, and Pineau}}}
\bibcite{manning2014stanford}{{55}{2014}{{Manning et~al.}}{{Manning, Surdeanu, Bauer, Finkel, Bethard, and McClosky}}}
\bibcite{mnih2008empirical}{{56}{2008}{{Mnih et~al.}}{{Mnih, Szepesv{'{a}}ri, and Audibert}}}
\bibcite{nallapati2016abstractive}{{57}{2016}{{Nallapati et~al.}}{{Nallapati, Zhou, Gulcehre, Xiang et~al.}}}
\bibcite{nguyen2016ms}{{58}{2016}{{Nguyen et~al.}}{{Nguyen, Rosenberg, Song, Gao, Tiwary, Majumder, and Deng}}}
\bibcite{novikova2017why}{{59}{2017}{{Novikova et~al.}}{{Novikova, Du\v {s}ek, Curry, and Rieser}}}
\bibcite{owen2013monte}{{60}{2013}{{Owen}}{{}}}
\bibcite{pagliardini2017unsupervised}{{61}{2017}{{Pagliardini et~al.}}{{Pagliardini, Gupta, and Jaggi}}}
\bibcite{paisley2012variational}{{62}{2012}{{Paisley et~al.}}{{Paisley, Blei, and Jordan}}}
\bibcite{papineni02bleu}{{63}{2002}{{Papineni et~al.}}{{Papineni, Roukos, Ward, and Zhu}}}
\bibcite{passonneau2014benefits}{{64}{2014}{{Passonneau and Carpenter}}{{}}}
\bibcite{paulus2018deep}{{65}{2018}{{Paulus et~al.}}{{Paulus, Xiong, and Socher}}}
\bibcite{pavlick2016gun}{{66}{2016}{{Pavlick et~al.}}{{Pavlick, Ji, Pan, and Callison-Burch}}}
\bibcite{ranganath2014black}{{67}{2014}{{Ranganath et~al.}}{{Ranganath, Gerrish, and Blei}}}
\bibcite{ratinov2011local}{{68}{2011}{{Ratinov et~al.}}{{Ratinov, Roth, Downey, and Anderson}}}
\bibcite{reddy2014large}{{69}{2014}{{Reddy et~al.}}{{Reddy, Lapata, and Steedman}}}
\bibcite{ripley2009stochastic}{{70}{2009}{{Ripley}}{{}}}
\bibcite{sakai2008information}{{71}{2008}{{Sakai and Kando}}{{}}}
\bibcite{sculley2007online}{{72}{2007}{{Sculley}}{{}}}
\bibcite{see2017point}{{73}{2017}{{See et~al.}}{{See, Liu, and Manning}}}
\bibcite{settles2010active}{{74}{2010}{{Settles}}{{}}}
\bibcite{snover2006ter}{{75}{2006}{{Snover et~al.}}{{Snover, Dorr, Schwartz, Micciulla, and Makhoul}}}
\bibcite{socher2013recursive}{{76}{2013}{{Socher et~al.}}{{Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts}}}
\bibcite{tan2018s}{{77}{2018}{{Tan et~al.}}{{Tan, Wei, Yang, Lv, and Zhou}}}
\bibcite{vannella2014validating}{{78}{2014}{{Vannella et~al.}}{{Vannella, Jurgens, Scarfini, Toscani, and Navigli}}}
\bibcite{vedantam2015cider}{{79}{2015}{{Vedantam et~al.}}{{Vedantam, Zitnick, and Parikh}}}
\bibcite{webber2010measurement}{{80}{2010}{{Webber}}{{}}}
\bibcite{weissenborn2017fastqa}{{81}{2017}{{Weissenborn et~al.}}{{Weissenborn, Wiese, and Seiffe}}}
\bibcite{wu2016google}{{82}{2016}{{Wu et~al.}}{{Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao, Gao, Macherey et~al.}}}
\bibcite{yilmaz2008simple}{{83}{2008}{{Yilmaz et~al.}}{{Yilmaz, Kanoulas, and Aslam}}}
\bibcite{zobel1998reliable}{{84}{1998}{{Zobel}}{{}}}
