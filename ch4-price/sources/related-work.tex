\section{\label{sec:setup} Related work}

In this work, we focus on using existing automatic metrics to decrease the cost of human evaluations.
There has been much work on improving the quality of automatic metrics.
In particular, there is interest in learning models~\citep{lowe2017towards,dusek2017referenceless} that are able to optimize for improved correlations with human judgment.
However, in our experience, we have found that these learned automatic metrics have trouble generalizing to different systems.
The framework we provide allows us to safely incorporate such models into evaluation, exploiting them when their correlation is high but also not introducing bias when it is low.

Our key technical tool is control variates, a standard statistical technique used to reduce the variance of Monte Carlo estimates~\citep{ripley2009stochastic}.
The technique has also been used in machine learning and reinforcement learning to lower variance estimates of gradients~\citep{greensmith2004variance, paisley2012variational, ranganath2014black}.
To the best of our knowledge, we are the first to apply this technique in the context of language evaluation.

Our work also highlights the importance of human evaluation.
\citet{chaganty2017unbiased} identified a similar problem of systematic bias in evaluation metrics in the setting of knowledge base population and also propose statistical estimators that relies on human evaluation to correct bias.
Unfortunately, their technique relies on having a structured output (relation triples) that are shared between systems and does not apply to evaluating natural language generation.
In a similar vein, \citet{chang2017affordable} dynamically collect human feedback to learn better dialog policies.



