\section{\label{sec:bias} Bias in automatic evaluation}




\begin{table*}
  \begin{subtable}{\textwidth}
  \centering
  \input{examples-1.table}
  \caption{\textbf{MS MARCO.} Human annotators rated answer correctness (\texttt{AnyCorrect}) and the automatic metric used is ROUGE-L (higher is better).}
  \end{subtable} \vspace{1em} \\
  \begin{subtable}{\textwidth}
  \centering
  \input{examples-2.table}
  \caption{\textbf{CNN/Daily Mail.} Human judgment scores used are post-edit distance (\texttt{Edit}) (lower is better) and the automatic metric used is sentence vector similarity with the reference (higher is better).}
  \end{subtable}
  \caption{\label{tab:examples}
    Examples highlighting the different modes in which the automatic metric and human judgments may agree or disagree.
    On the MS MARCO task, a majority of responses from systems were actually correct but poorly scored according to ROUGE-L.
    On the CNN/Daily Mail task, a significant number of examples which are scored highly by VecSim are poorly rated by humans, and likewise many examples scored poorly by VecSim are highly rated by humans.
  }
\end{table*}

It is well understood that current automatic metrics tend to correlate poorly with human judgment at the instance-level.
For example, \citet{novikova2017why} report correlations less than $0.3$ for a large suite of word-based and grammar-based evaluation methods on a generation task.
Similarly, \citet{liu2016evaluate} find correlations less than $0.35$ for automatic metrics on a dialog generation task in one domain, but find correlations with the same metric dropped significantly to less than $0.16$ when used in another domain. 
Still, somewhat surprisingly, several automatic metrics have been found to have high \textit{system-level} correlations~\citep{novikova2017why}.
What, then, are the implications of having a low instance-level correlation?  

As a case study, consider the task of open-response question answering:
  here, a system receives a human-generated question and must \textit{generate} an answer from some given context, e.g.\ a document or several webpages.
  We collected the responses of several systems on the MS MARCOv1 dataset~\citep{nguyen2016ms} and crowdsourced human evaluations of the system output
  (see \refsec{tasks} for details).

The instance-level correlation (\reffig{bias-msmarco-instance}) is only $\rho = 0.31$.
A closer look at the instance-level correlation reveals that
while ROUGE is able to correctly assign low scores to bad examples (lower left),
it is bad at judging good examples and often assigns them low ROUGE scores (lower right)---see \reftab{examples} for examples.
This observation agrees with a finding reported in \citet{novikova2017why} that automatic metrics correlate better with human judgments on bad examples than average or good examples. 


Thus, as \reffig{bias-msmarco}(a) shows, we can improve low-scoring ROUGE examples without improving their human judgment ($\vartriangle$) and vice versa ($\triangleright$).
Indeed, \citet{conroy2008mind} report that summarization systems were optimized for ROUGE during the DUC challenge~\citep{dang2006overview}
until they were indistinguishable from the ROUGE scores of human-generated summaries, but the systems had hardly improved on human evaluation.
Hill-climbing on ROUGE can also lead to a system that does worse on human scores, e.g.\ in machine translation~\citep{wu2016google}.
Conversely, genuine quality improvements might not be reflected in improvements in ROUGE\@.
This bias also appears in pool-based evaluation for knowledge base population \citep{chaganty2017unbiased}.
Thus the problems with automatic metrics clearly motivate the need for human evaluation,
but can we still use the automatic metrics somehow to save costs?






