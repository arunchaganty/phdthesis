\section{\label{sec:models} Evaluation models}
\pl{use term 'surrogate'}
In the previous section, we have observed that the data efficiency of using a model in evaluation is fundamentally limited by the model's correlation with ground truth, as well as annotator variance.
We found that, in theory, with a perfect model, it should be possible halve or even reduce annotation costs by a third.
In this section, we look at a few different evaluation models and report their performance in practice.

Note that while we should still expect evaluation modeling to be difficult, we should hope that it is easier than learning the task itself as we have access not only to the generated utterance but also to existing references that have been rated.

\paragraph{Word-overlap based metrics}
As baseline models we consider using standard automatic metrics such as BLEU, ROUGE, etc.

\paragraph{Sentence embedding models}
Inspired by ADEM, we consider using sentence embedding models and learn the following linear relation between ratings on references:
$g(x,y) \propto \sum_{y_r} f(y_r) * y^\top M y_r$

\paragraph{Task-specific model for MSMarco}
The MSMarco task has the most opportunity for improvements, so we augment a good architecture for the task.

\paragraph{Pretraining}
For each task, we have too little data to learn an entire model, embeddings, so we focus on using the data to fine-tune the model to improve it's correlation. 
To learn the embeddings, we use pretrained models.

Note that we are able to use the labeled data to train our models as long as we use its prediction only on future data. 

\paragraph{Learning optimal scaling using ordinal regression}
Learn how to weigh above features to match ordinal / real-valued ratings.
