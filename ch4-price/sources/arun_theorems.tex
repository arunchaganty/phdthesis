\onecolumn

\section{\label{sec:proofs}Proofs}

In this section, we provide proofs for the theorems stated in the main paper.
As many of these proofs are general in nature, we adopt a change of notation to be closer to those used in statistics. 

\subsection{Unbiased estimation.}

Let $x_i \in \sX$ be a set of (ground truth) variables,
let $y_i \eqdef x_i + \s_Y\e_i$ where $e_i$ be some independent zero-mean random variable,
and let $z_i$ be another sequence of random variables with no particular conditions.

A system $S$ comprises of a subset of these indices, $\{x_i: i \in S\} \subseteq \sX$.
Let $\Xb_S = \frac{1}{S} \sum_{i \in S} x_i$ be the average system performance.
Our goal is, given systems $S_1, S_2, \dots$, access to $Y_1, \dots$ and $Z_1, \dots$ is to estimate $\bXb = \Xb_1, \Xb_2, \dots$. \ac{express that $Y_i$ doesn't need to span all samples.}
Furthermore, let the correlation between $X_i$ and $Z_i$ be $\rho_i$.

Let $\delta(\bY, \bZ)$ be an estimator of $\bXb$.
Unbiasedness requires that $\E[\delta(\bY, \bZ)] = \bXb$.
For concreteness, the risk of this estimator is $\E[\|\delta(\bY, \bZ) - \bXb\|^2]$
In this case, a minimum risk estimator using the squared loss is also the minimum variance estimator.

\paragraph{Simple estimator}
Let $\delta(\bY, \bZ) \eqdef (\bYb_1, \dots, \bYb_n)$.
$\delta$ is unbiased.
The variance of $\delta_i$ is:
\[
\Var(\delta_i) = \frac{\s^2_{X_i} + \s^2_Y}{n_i}.
\]

\ac{Technically Stein estimators can be better for $n > 3$ systems\dots}

\paragraph{Simple model-baseline estimator}
Let $\delta(\bY, \bZ) \eqdef (\bZb_1 + \bYb_1 - \bZb_1, \dots)$. \ac{here, the second $\bZb_i$ is restricted to what's been annotated.}

The variance of $\delta_i$ is:
\[
\Var(\delta_i) = \frac{(1-\rho_i^2)\s^2_{X_i} + \s^2_Y}{n_i}.
\]

\paragraph{Optimal model-based estimators}

Let us further assume that both $y_i$ (really $e_i$) and $z_i$ are independent and normally distributed;
  in this scenario, $T = (\bYb, \bZb, S_y, S_z)$ are complete sufficient statistics.
Furthermore, by the magical property of Gaussians, each of these terms is independent of the other.

By Rao-Blackwell, we know that any unbiased function that is only a function of $T$ is optimal.
Let $\delta = \bYb - \bZb'$.

\ac{Stein's shows up here too, but in that case, we consider things that could be unbiased too.}

\section{\label{sec:incomplete}Incomplete}

This section is development space for new material.

\subsection{Incorporating annotator variance}
Broader formulation of annotator variance.

\subsection{Equivariant formulation}
In principle, unbiasedness is too strong a condition for our evaluation metric because it is not important if the metric has been scaled and shifted.
In other words, if $F_i$ is the quality of system $S_i$, then knowing $F'_i \gets \alpha F_i + \beta$ would be just as good, as long as each $F'_i$ respected the same linear transformation.

More formally, let us say that the output $z$ is sampled according to some (unknown) distribution assume $p(z; F)$, then $p(z\given F) = p(\alpha z + \beta \given \alpha F + \beta)$.
$f_i \to \alpha f_i + \beta$ if $F \to \alpha F + \beta$.

A more appropriate formulation then is that of \textit{equivariant estimation}:
(define equivariance).

In this formulation, let $f_i \eqdef f(x_i)$ be the ground truth for $f_i$, and $h_i \eqdef \alpha f_i + \beta + \epsilon$, where $\epsilon$ is some zero mean residual, be human responses.
In other words, the ground truth is \textit{defined} to be some scale/shift of the mean human response: this scaling/shifting is constant across examples.
Without loss of generality, $\alpha = 1$ and $\beta = 0$.

Our question then is, given a series of responses $g_i$, $h_i$ (where $g_i$ comes from the model), what is the minimum risk equivariant (MRE) for $F = \E[f_i]$?
Here, the risk function instead of being squared loss, is $\ell(\frac{\delta - \beta}{\alpha})$ or something like that that is invariant to scaling and shifting in $F$ (as those would result in an appropriate shifting/scaling of $\alpha$ and $\beta$).

\section{Statistical lemmas}

\begin{lemma}[Sufficient statistics for a two-factor Gaussian,unknown mean,  known covariance]%
\label{lem:suff-stats-two-factor}
  Let $y_{uv} = x_u + w_v + r_{uv}$ be a two-factor Gaussian model where $x_u \sim \sN(\mu_X, \sigma_X^2)$, $w_v \sim \sN(0, \sigma_W^2)$ and $r_{uv} \sim \sN(0, \sigma_{R}^2)$ are all drawn independently.
  The covariance of $y_{uv}$ is given by $\Sigma$. 

  Then, the sufficient statistic for the distribution of $\by \given \mu_X$ is $\ones^\top\Sigma^{-1} \by$.
\end{lemma}
\begin{proof}
  We use the Fisher-Neymann factorization theorem:
  \begin{align*}
  \log p(\by \given \mu_X) 
      &= C - \frac{1}{2} (\by - \mu_X \ones)^\top \Sigma^{-1}  (\by - \mu_X \ones) \\
      &= C'(\by) + C''(\mu_X) + (\by^\top \Sigma^{-1} \mu_X \ones) \\
      &= C'(\by) + C''(\mu_X) + \mu_X (\by^\top \Sigma^{-1} \ones) \\
      &= C'(\by) + C''(\mu_X) + \mu_X (T(\by)).
  \end{align*}
  Thus, $T(\by) \eqdef \ones^\top\Sigma^{-1} \by$ is a (complete) sufficient statistic.
  \ac{Show that it is complete.}
\end{proof}

\begin{lemma}[Sufficient statistics for a restricted two-factor Gaussian, unknown mean, unknown covariance]%
\label{lem:suff-stats-two-factor-restricted}

  Similar to \reflem{suff-stats-two-factor},
  let $y_{uv} = x_u + w_v + r_{uv}$ be a two-factor Gaussian model where $x_u \sim \sN(\mu_X, \sigma_X^2)$, $w_v \sim \sN(0, \sigma_W^2)$ and $r_{uv} \sim \sN(0, \sigma_{R}^2)$ are all drawn independently.
  The covariance of $y_{uv}$ is given by $\Sigma$. 
  We assume further structure on $\Sigma$.

  Suppose $y_{uv}$ is the $i$-th response, then
  let $U$ be an indicator matrix where each row $i$ has a single non-zero unit element at the index $u$ and let $V$ be an indicator matrix such row $i$ its a single non-zero unit element at index $v$.
  Further, let we assume that the columns of $U$ are also unique; without loss of generality, $U = I$.
  Responses can still be correlated because they share $w_v$ terms, given by $V$.

  Then, the sufficient statistics for the distribution of $\by \given \mu_X, \Sigma$ are:
  \begin{align*}
    T_1(\by) &= \|\by\|^2 \\
    T_2(\by) &= \|V^\top \by\|^2 \\
    T_3(\by) &= \ones^\top \by \\
    T_4(\by) &= \ones^\top V V^\top \by.
  \end{align*}
\end{lemma}
\begin{proof}
  Note that in this case, the covariance matrix $\Sigma = \sigma^2_X + \sigma^2_R I + \sigma^2_W V V^\top$.

  From \reflem{indicator-matrix}, we have that: 
  \begin{align*}
    \Sigma^{-1} 
      &= ((\sigma^2_X + \sigma^2_R) I + \sigma^2_W V V^\top)^{-1} \\
      &= D^{-1}(I + D' V V^\top)^{-1} \\
      &= D^{-1} I - D^{-1} \frac{D'}{1+D'} V V^\top \\
      &= C I - C' V V^\top,
  \end{align*}
  where it should be noted that $C$ and $C'$ are functions of the unknown parameters $\sigma^2_X$, $\sigma^2_W$ and $\sigma^2_R$.

  Once again, we use the Fisher-Neymann factorization theorem:
  \begin{align*}
  \log p(\by \given \mu_X, \Sigma) 
      &= D(\Sigma) - \frac{1}{2} (\by - \mu_X \ones)^\top \Sigma^{-1}  (\by - \mu_X \ones) \\
      &= D(\Sigma) - \frac{1}{2} (\by - \mu_X \ones)^\top (CI - C' VV^\top) (\by - \mu_X \ones) \\
      &= D'(\mu_X, \Sigma) - \frac{1}{2} (C \|\by\|^2 + 2 C' \|\by V\|^2 - 2 \mu_X \ones^\top \by - 2 \mu_X C' \ones^\top V V^\top \by)\\
      &= D'(\mu_X, \Sigma) - \frac{1}{2} f(T_1(\by), T_2(\by), T_3(\by), T_4(\by)),
  \end{align*}
  where 
    $T_1(\by) = \|\by\|^2$,
    $T_2(\by) = \|V^\top \by\|^2$,
    $T_3(\by) = \|\ones^\top \by\|^2$,
    $T_4(\by) = \|\ones^\top V V^\top \by\|^2$.

  \ac{To show that these are complete.}
\end{proof}

\begin{lemma}[Properties of indicator matrices]%
\label{lem:indicator-matrix}

  An indicator matrix is a matrix such that each row contains exactly one non-zero element which is equal to one.
  Let $U$ be a $m\times n$ indicator matrix where $u(i) \in [n]$ be the index of the unit entry in the $i$-th row.
  Similarly, let $V$ be a $m\times p$ indicator block matrix where $v(i) \in [p]$ is analogously defined.
  Then, the following properties hold:
  \begin{enumerate}
    \item ${(UU^\top)}_{ij} = \I[u(i) = u(j)]$.
    \item ${(U^\top U)} = I$.
    \item $W \eqdef {(U^\top V)}_{ij} = \sum_{k} \I[u(k) = i \band v(k) = j]$.
    \item $W^\top W = \sum{k', k''} \I[u(k') = u(k'')] \I[v(k') = i] \I[v(k'') = j]$.
    \item $W W^\top = \sum{k', k''} \I[v(k') = v(k'')] \I[u(k') = i] \I[u(k'') = j]$.
    \item ${(I + k UU^\top)}^{-1} = I - \frac{k}{k+1} U U^\top$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  The first four results follow from elementary matrix multiplication:
  \begin{align*}
    {(UU^\top)}_{ij}
    &= \sum_{k} U_{ik} U_{jk} \\
    &= \sum_{k} \I[k = u(i)] \I[k = u(j)] \\
    &= \I[u(i) = u(j)] \\
    {(U^\top U)}_{ij}
    &= \sum_{k} U_{ki} U_{kj} \\
    &= \sum_{k} \I[u(k) = i] \I[u(k) = j] \\
    &= \I[i = j] \\
    W 
    &= \sum_{k} U_{ki} V_{kj} \\
    &= \sum_{k} \I[u(k) = i] \I[v(k) = j] \\
    W^\top W
    &= \sum_{k} W_{ki} W_{kj} \\
    &= \sum_{k} \sum_{k'} \I[u(k') = k] \I[v(k') = i] \sum_{k''} \I[u(k'') = k] \I[v(k'') = j] \\
    &= \sum_{k', k''} \I[u(k') = u(k'')] \I[v(k') = i] \I[v(k'') = j] \\
    W W^\top
    &= \sum_{k} W_{ik} W_{jk} \\
    &= \sum_{k} \sum_{k'} \I[u(k') = i] \I[v(k') = k] \sum_{k''} \I[u(k'') = i] \I[v(k'') = k] \\
    &= \sum_{k', k''} \I[v(k') = v(k'')] \I[u(k') = i] \I[u(k'') = j].
  \end{align*}

  The final result follows from the Woodbury matrix identity: 
  \begin{align*}
    {(I + k U U^\top)}^{-1}
    &= I - U {(\frac{1}{k} I +  U^\top U)}^{-1} U^\top \\
    &= I - U {(\frac{1}{k} I + I)}^{-1} U^\top \\
    &= I - \frac{k}{k+1} U U^\top.
  \end{align*}
\end{proof}

\begin{lemma}[Woodbury Matrix Identity]
  Let $A, U, C, V$ be matrix of appropriate dimensions: ${\left(A+UCV\right)}^{-1} = A^{-1} - A^{-1} U {\left(C^{-1}+VA^{-1}U\right)}^{-1} V A^{-1}$. 
\end{lemma}

\begin{lemma}[Non-independence]
  If $U$ and $V$ are indicator matrices then ${(I + k UU^\top + l VV^\top)}^{-1}$ can not be written as linear sequence $\sum_{k,l} f_i(k,l) g_i(U, V)$.
\end{lemma}
\begin{proof}
  Let $k' = \frac{k}{k+1}$.
  \begin{align*}
    {(I + k U U^\top + l VV^\top)}^{-1}
    &= I - k' U U^\top - (I - k' U U^\top) V (\frac{1}{l} I + V^\top (I - k' U U^\top) V)^{-1} V^\top (I - k' U U^\top) \\
    &= I - k' U U^\top - (V - k' U W) (\frac{1}{l} I + I - k' W^\top W)^{-1} (V - k' U W)^\top \\ 
    &= I - k' U U^\top - l' (V - k' U W) (I - l' k' W^\top W)^{-1} (V - k' U W)^\top.
  \end{align*}
  Unfortunately $W^\top W$ can not be further simplified. The best we can do is to write out a Taylor expansion:
  \[
  (I - l' k' W^\top W)^{-1} = 
  I + l' k' W^\top W + (l' k')^2 (W^\top W)^2 + \ldots.
  \]
  Additionally, $l' k' W^\top W$ is typically not small and so we need lots of terms for this series to be useful.
\end{proof}
