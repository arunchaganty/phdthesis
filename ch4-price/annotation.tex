\section{\label{sec:price:interfaces} Crowdsourcing data collection}

In this section, we provide details regarding our the design of our annotation interfaces and the quality control measures we took.

\subsection{Language quality evaluation.}
Each human annotator was shown a short summary that was generated by a system from an article in the CNN/Daily Mail dataset or provided as a reference for that article.
The annotators were then asked to (a) provide Likert scale ratings of the summary on multiple facets (fluency, redundancy and overall quality) and (b) perform post-edits to correct any errors (\reffig{price:lqual-interface}).

\subsection{Answer correctness evaluation.}
Each annotator was shown a question from the MS MARCO dataset and an answer that was generated by a system or provided as a reference answer from the dataset.
The annotators were then asked to (a) rate if the question made sense and the answer was plausibly correct and (b) asked to identify which paragraphs provided in the dataset justified the answer (\reffig{price:msmarco-interface}).

%\subsection{Round-trip machine translation quality}
%For an independent source of language quality evaluations, we used the annotations collected by \citet{lau2017grammaticality} to evaluate the level of ``acceptability'' of a sentence.
%This dataset contains of 2,500 sentences derived from the BNC Consortium (2007) and 2,500 sentences from Wikipedia.
%Each sentence was then translated into one of four languages (Spanish, Norwegian, Chinese and Japanese) using Google Translate and then translated back into English to be evaluated by many different human annotators.
%It is observed that performing round-trip machine translation through different languages degrades the quality of of the resulting sentence to different degrees.
%For purposes of this benchmark, we consider each language as a separate system for evaluation.
%
