\section{Statistical estimation for unbiased evaluation}
\label{sec:method}

We will now formalize the problem of combining human evaluation with an automatic metric.
Let $\sX$ be a set of inputs (e.g., articles),
and let $S$ be the \emph{system} (e.g.\ for summarization),
which takes $x \in \sX$ and returns output $S(x)$ (e.g.\ a summary).
Let $\sZ = \{ (x, S(x)) : x \in \sX \}$ be the set of system predictions.
Let $Y(z)$ be the random variable representing the human judgment according to some evaluation prompt (e.g.\ grammaticality or correctness),
and define $f(z) = \E[Y(z)]$ to be the (unknown) \emph{human metric} corresponding to averaging over an infinite number of human judgments.
Our goal is to estimate the average across all examples:
\begin{align}
\mu \eqdef \E_z[f(z)] = \inv{|\sZ|} \sum_{z \in \sZ} f(z)
\end{align}
with as few queries to $Y$ as possible.

\pl{using the $S$ notation is good, try to incorporate this into previous chapters}

Let $g$ be an automatic metric (e.g. ROUGE), which maps $z$ to a real number.
We assume evaluating $g(z)$ is free.
The central question is how to use $g$ in conjunction with calls to $Y$ to produce an unbiased estimate $\hat\mu$ (that is, $\E[\hat\mu] = \mu$).
In this section, we will construct a simple estimator based on control variates~\citep{ripley2009stochastic},
and prove that it is minimax optimal.

%In many settings, sampling from $Y(z)$ costs resources (e.g. paying annotators) so we wish to minimize the number of queries to $Y(z)$.
%On the other hand, we would like a good estimate of $\mu_Y$, which fundamentally requires queries.
%We hope to gain in data efficiency by using an automatic metric that is correlated with the mean human response.
%The naive empirical mean only uses samples from $Y(z)$, while we provide a
%control variates estimator that both uses the samples from $Y(z)$ and the
%associated free-to-compute $g(z)$ values. Our estimator is both surprisingly
%simple and minimax optimal.

%In this work, we study the problem of estimating the mean human score of a particular system with the aid of an automatic metric that is correlated with the mean score. 

%In fact, under an uncorrelated human response assumption (not necessarily independent or identically distributed) and a correlated automatic metric, we provide an unbiased estimator with the minimax optimal variance (among unbiased estimators).
%The estimator is surprisingly simple and easy to compute. The relative efficiency of the estimator is influenced by two key quantities: $\rho$, the correlation between the automatic metric and the human metric, and $\gamma$, the relative human response variance.

%For this work, our objective will be to have an unbiased estimate with a low variance with the lowest resource cost.

\subsection{Sample mean}

We warm up with the most basic unbiased estimate, the sample mean.
We sample $z^{(1)}, \dots, z^{(n)}$ independently with replacement from $\sZ$.
Then, we sample each human judgment $y^{(i)} = Y(z^{(i)})$ independently.\footnote{%
Note that this independence assumption isn't quite true in practice since we do not control who annotates our data.}
Define the estimator to be $\musimple = \frac{1}{n} \sum_{i=1}^n y^{(i)}$.
Note that $\musimple$ is unbiased ($\E[\musimple] = \mu$).

We can define $\sigma^2_f \eqdef \Var(f(z))$ as the variance of the human metric
and $\sigma^2_a \eqdef \E_z[\Var(Y(z))]$ as the variance of human judgment averaged over $\sZ$.
By the law of total variance, the variance of our estimator
is
\begin{align}
\label{eqn:varsimple}
\Var(\musimple) = \frac{1}{n} (\sigma^2_f + \sigma^2_a).
\end{align}
% PL: too much
%Furthermore, $\musimple$ has the minimax optimal
%variance among unbiased estimators that are functions of $\{y^{(i)}\}$.
% This is the typical way of estimating means and we will use it as the baseline in this work.
%\stm{proposition? reference? obvious? I could use this as a gentle warm-up for the later minimax proof arguments.}

\pl{maybe the annotator variance should be introduced in the background chapter...;
or if not, at least relate to the previous chapter}

\subsection{Control variates estimator}

% Intuition
Now let us see how an automatic metric $g$ can reduce variance.
If there is no annotator variance ($\sigma^2_a = 0$) so that $Y(z) = f(z)$,
we should expect the variance of $f(z)-g(z)$ to be lower than the variance of
$f(z)$, assuming $g$ is correlated with $f$---see \reffig{variance_reduction} for an illustration.

The actual control variates estimator needs to
handle noisy $Y(z)$ (i.e.\ $\sigma^2_a > 0$) and
guard against a $g(z)$ with low correlation.
% Actual estimator
Let us standardize $g$ to have zero mean and unit variance, because we have
assumed it is free to evaluate.
As before, let $z^{(1)}, \dots, z^{(n)}$ be independent samples from $\sZ$ and
draw $y^{(i)} = Y(z^{(i)})$ independently as well.
We define the \emph{control variates estimator} as
\begin{align}
\mucontrol = \frac{1}{n} \sum_{i=1}^n y^{(i)} - \alpha g(z^{(i)}),
\end{align}
where
\begin{align}
  \alpha \eqdef \Cov(f(z),g(z)).
\end{align}
Intuitively, we have averaged over $y^{(i)}$ to handle the noise introduced by $Y(z)$, and scaled $g(z)$ to prevent an uncorrelated automatic metric from introducing too much noise.

An important quantity governing the quality of an automatic metric $g$
is the correlation between $f(z)$ and $g(z)$ (recall that $g$ has unit variance):
\begin{align}
\rho \eqdef \frac{\alpha}{\sigma_f}.  % PL: remove \sigma_g = 1
\end{align}

\begin{figure}
\centering
  \includegraphics[width=0.8\columnwidth]{figures/variance_reduction_extreme}
  \caption[Intuition for control variates]{\label{fig:variance_reduction}
  The samples from $f(z)$ have a higher variance than the samples
  from $f(z)-g(z)$ but the same mean. This is the key idea behind using control variates to reduce variance.}
\end{figure}

% Minimax optimal
We can show that among all distributions with
fixed $\sigma^2_f$, $\sigma^2_a$, and $\alpha$ (equivalently $\rho$), this estimator is minimax optimal, i.e.\ it has the least variance among all unbiased estimators:

\begin{theorem}
\label{thm:main}
Among all unbiased
  estimators that are functions of $y^{(i)}$ and $g(z^{(i)})$, and for all distributions with a given $\sigma^2_f$, $\sigma^2_a$, and $\alpha$,
\begin{align}
  \label{eqn:varcontrol}
  \Var(\mucontrol) = \frac{1}{n} (\sigma^2_f (1 - \rho^2) + \sigma^2_a),
\end{align}
and no other estimator has a lower worst-case variance.
\end{theorem}

%We typically will choose $n$ so that the sample variance is below some
%threshold.
%For both the empirical mean estimator and the control variates
%estimator, the variance goes as $\Theta(1/n)$ which means that the ratio of the
%sample complexities is the same as the ratio of the variances.
%Further, the
Comparing the variances of the two estimators (\refeqns{varsimple}{varcontrol}),
we define the \emph{data efficiency} as the ratio of the variances:
\begin{align}
\DE \eqdef \frac{\Var(\musimple)}{\Var(\mucontrol)} = \frac{1 + \gamma}{1-\rho^2 + \gamma},
\end{align}
where $\gamma \eqdef \sigma^2_a / \sigma^2_f$ is the normalized annotator variance.
Data efficiency is the key quantity in this paper:
  it is the multiplicative reduction in the number of samples required
  when using the control variates estimator $\mucontrol$ versus the sample mean $\musimple$.
Figure~\ref{fig:savings} shows the inverse data efficiency contours as a function of the correlation $\rho$
and $\gamma$.

\begin{figure}
\centering
  \includegraphics[width=0.8\columnwidth]{figures/savings}
  \caption[Inverse data efficiency with the control variates estimator]  {\label{fig:savings} Inverse data efficiency for various values of
  $\gamma$ and $\rho$.  We need both low $\gamma$ and high $\rho$ to obtain
  significant gains.
  %Note that we do not even get a data efficiency of $2$ for
  %the darkest 5 stripes.
  % PL: removed it, if want to include it, need to explain better
  }
\end{figure}

When there is no correlation between human and automatic metrics ($\rho = 0$),
the data efficiency is naturally $1$ (no gain).
In order to achieve a data efficiency of
$2$ (half the labeling cost), we need $|\rho| \geq \sqrt{2}/2 \approx 0.707$.
Interestingly, even for an automatic metric with perfect correlation ($\rho=1$),
the data efficiency is still capped by $\frac{1 + \gamma}{\gamma}$:
unless $\gamma \to 0$ the data efficiency cannot increase unboundedly.
Intuitively, even
if we knew that $\rho=1$, $f(z)$ would be undetermined up to a
constant additive shift and just estimating the shift would incur a variance of $\frac{1}{n} \sigma_a^2$.
%\footnote{This is the case if the variance of $f$ at the single point is the average variance over all $z$.}

%At the same time, the higher the relative normalized annotator judgment variance $\gamma$, the worse
%the data efficiency gains.

%It is important to note that in order to use this estimator, we need to know
%the value of $\alpha = Cov(f(z),g(z))$. Later, we will show that estimating
%$\hat{\alpha}$ with the same sample introduces a bias and increase in variance
%that are low-order terms in $n$. However, the minimax optimal variance
%provides a lower bound in the cases where we don't know $\alpha$.

% $f(z)$ could be shifted In the case that $\rho=1$, $$f(z) - \mu_f = \frac{\sigma_f}{\sigma_g} (g(z) - \mu_g)$$ In the best case, when there exists a $z_0$ such that $g(z_0) = \mu_g$, $\mu_f = f(z_0)$. It seems like we would be done at this point, except that even to estimate $f(z_0)$ with $n$ samples incurs a variance of $\frac{1}{n} \sigma_a^2$ \footnote{this is only the case if the variance of $f$ at $z_0$ is the average variance overall all $z$}. Taking the ratio with the naive estimate yields the predicted $\frac{1 + \gamma}{\gamma}$.

\subsection{Using the control variates estimator}
The control variates estimator can be easily integrated into an existing evaluation:
we run human evaluation on a random sample of system outputs, automatic evaluation on all the system outputs, and plug in these results into \refalg{estimate}.

It is vital that we are able to evaluate the automatic metric on a significantly larger set of examples than those with human evaluations to reliably normalize $g(z)$:
without these additional examples, it be can shown that the optimal minimax estimator for $\mu$ is simply the naive estimate $\musimple$.
Intuitively, this is because estimating the mean of $g(z)$ incurs an equally large variance as estimating $\mu$.
In other words, $g(z)$ is only useful if we have additional information about $g$ beyond the samples $\{z^{(i)}\}$.

\refalg{estimate} shows the estimator.
In practice, we do not know $\alpha = \Cov(f(z),g(z))$, so we use a plug-in estimate $\hat{\alpha}$ in line 3 to compute the estimate $\widetilde{\mu}$ in line 4.
We note that estimating $\alpha$ from data does introduce a $O(1/n)$ bias,
but when compared to the standard deviation which decays as $\Theta(1/\sqrt{n})$, this bias quickly goes to $0$.
%Similarly, the variance is sub-optimal by a factor of only $1 + O(1/n)$.

\begin{proposition}
\label{prop:added_bias}
The estimator $\widetilde{\mu}$ in \refalg{estimate} has $O(1/n)$ bias.
\end{proposition}

\begin{algorithm}
      \caption{\label{alg:estimate}Control variates estimator}
      \begin{algorithmic}[1]
   \State{} {\bfseries Input:} $n$ human evaluations $y^{(i)}$ on system outputs $z^{(i)}$, \textit{normalized} automatic metric $g$ 
   \State{} $\overline{y} = \frac{1}{n} \sum_i y^{(i)}$
   \State{} $\hat{\alpha} = \frac{1}{n} \sum_i (y^{(i)} - \overline{y}) g(z^{(i)})$
   \State{} $\widetilde{\mu} = \frac{1}{n} \sum_i y^{(i)} - \hat{\alpha} g(z^{(i)})$
   \State{} {\bfseries return} $\widetilde{\mu}$
\end{algorithmic}
\end{algorithm}

An additional question that arises when applying \refalg{estimate} is figuring out how many samples $n$ to use.
Given a target variance, the number of samples can be estimated using \refeqn{varcontrol} with conservative estimates of $\sigma^2_f$, $\sigma^2_a$ and $\rho$.
Alternatively, our estimator can be combined with a dynamic stopping rule~\citep{mnih2008empirical} to stop data collection once we reach a target confidence interval.

\subsection{Discussion of assumptions}
We will soon see that empirical instantiations of $\gamma$ and $\rho$ lead to rather underwhelming data efficiencies in practice.
In light of our optimality result, does this mean there is no hope for gains?
%Modulo the asymptotically vanishing bias caused by estimating $\alpha$
%unfortunately, our theory states no other unbiased estimator with access to only human feedback and an automatic metric can do better.
Let us probe our assumptions.
We assumed that the human judgments are uncorrelated across different system outputs;
it is possible that a more accurate model of human annotators (e.g.\ \citet{passonneau2014benefits}) could offer improvements.
%\pl{really? isn't independent judgments the best we can hope for?}
% ^^ARUN: independent judgments are only the best choice under the naive model of human annotators we have which is that they are all interchangable and have iid noise. The intuition we have that turkers who label are lot are actually _better_ than those who label 1-2 examples is incongruent with  this simplistic model.
Perhaps with additional information about $g(z)$ such as calibrated confidence estimates,
we would be able to sample more adaptively.
Of course the most direct routes to improvement involve increasing the correlation of $g$ with human judgments and reducing annotator variance,
which we will discuss more later.


