%\begin{abstract}
For evaluating generation systems, automatic metrics such as BLEU, which cost nothing to run, have been used to score the output generated by a system based on its similarity with the reference answer provided with the test collection.
Unfortunately, these automatic metrics have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements.
On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive to run.

In the previous chapter, we tackled a similar problem in tasks with finite incompleteness by combining data collected across multiple systems and thus amortizing costs.
In this chapter, we'll look at a much harder problem, tackling \textit{infinite incompleteness} in evaluating generation systems:
in the infinite incompleteness setting, few predictions made by any two systems exactly overlap and thus the techniques of the previous chapter do not apply.

Instead, we use control variates to combine automatic metrics with human evaluation to
obtain an unbiased estimator with lower cost than human evaluation alone.
In practice, however, we obtain only a 7--13\% cost reduction on evaluating summarization and open-response question answering systems.
We then prove that our estimator is optimal: there is no unbiased estimator with lower cost.
Our theory further highlights the two fundamental bottlenecks---the automatic
metric and the prompt shown to human evaluators---both of which need to be improved to obtain greater cost savings.
