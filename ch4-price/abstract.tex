%\begin{abstract}
For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements.
On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive.
In this paper, we use control variates to combine automatic metrics with human evaluation to
obtain an unbiased estimator with lower cost than human evaluation alone.
In practice, however, we obtain only a 7--13\% cost reduction on evaluating summarization and open-response question answering systems.
We then prove that our estimator is optimal: there is no unbiased estimator with lower cost.
Our theory further highlights the two fundamental bottlenecks---the automatic
metric and the prompt shown to human evaluators---both of which need to be improved to obtain greater cost savings.

%Our theoretical guarantees show that any further cost reduction is fundamentally limited by the quality of current automatic metrics and annotation prompts:
  %improving the former has the potential to reduce costs by two-thirds and improving the latter has the potential to reduce costs by a third.
  % PL: not clear what these numbers are respect to
%\end{abstract}

% ACL submission version: 
%For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements.
%On the other hand, averaging human judgments is the gold standard but is often too expensive.
%Can we combine automatic metrics and human evaluation to obtain a score at lower cost without introducing bias?
%In this paper, we derive an optimal (i.e.\ minimum cost among all possible unbiased methods) unbiased estimator that uses only an automatic metric and human-in-the-loop feedback.
%We characterize its data efficiency ---the reduction factor in number of human judgments needed to obtain the same precision versus naive human evaluation--- in terms of the variance of human judgments and the correlation of the automatic metric with humans.
%On summarization and open-response question answering, we obtain only a 7--13\% reduction in cost.
%Our theoretical guarantees show that this modest cost reduction is fundamentally limited by the quality of current automatic metrics and annotation prompts: improving the former has the potential to reduce costs by $2/3$rds and improving the latter has the potential to reduce costs $1/3$rd.
%We are further able to show that improvements to the automatic metric or human annotation protocols have the potential to improve data efficiency to as much as 3x and 1.5x, respectively. This sheds light on the opportunities and limits of using automatic metrics to obtain a cheaper unbiased evaluation.
% NOTE: AC: these numbers are correct. ^^

  %the maximum cost savings is 2--3x.
%Our analysis shows that a
%  limiting factor is the high inter-annotator variance,
%  and thus we conclude that
%  developing better human evaluation protocols is as important as developing better automatic metrics
%  when scaling human evaluation.



%A significant challenge in developing generation systems has simply been evaluating their output.
%Most existing automatic metrics (e.g. BLEU, ROUGE, METEOR, etc.) have been shown to be poorly correlated with human judgment, as do learned metr
%Instead of seeking to develop a fully automatic evaluation metric that is able to consistently agree with human judgment across different syste
%We prove that no other approach using only human feedback and model predictions can do uniformly better than the one we propose.
%This allows us to characterize the ability to save costs during evaluation as dependent on having (a) low inter-annotator variance and (b) surrog
%With this theoretical footing, we evaluate our procedure on two tasks: evaluating the language quality of automatically generated summaries f
%We find that high inter-annotator variance caps cost savings to be at most 2x-3x and that current automatic metrics result in savings of only aro
%We conclude that the most important component of reducing costs at present is developing better evaluation protocols with lower inter-annotator  
