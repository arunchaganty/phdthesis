\section{\label{sec:models} Evaluation models}
\pl{use term 'surrogate'}
In the previous section, we have observed that the data efficiency of using a model in evaluation is fundamentally limited by the model's correlation with ground truth, as well as annotator variance.
We found that, in theory, with a perfect model, it should be possible halve or even reduce annotation costs by a third.
In this section, we look at a few different evaluation models and report their performance in practice.
%\ac{Fundamentally, this is going to be a negative report -- do we even have this section?}

Note that while we should still expect evaluation modeling to be difficult, we should hope that it is easier than learning the task itself as we have access not only to the generated utterance but also to existing references that have been rated.

% Baseline models to get a sense of the degree of correlation for each task.
\paragraph{Word-overlap based metrics}
As baseline models we consider using standard automatic metrics such as BLEU, ROUGE, etc.

% Sentence embedding models with the same sort of correlation thing that ADEM used. 
\paragraph{Sentence embedding models}
Inspired by ADEM, we consider using sentence embedding models and learn the following linear relation between ratings on references:
$g(x,y) \propto \sum_{y_r} f(y_r) * y^\top M y_r$

% More specific model for MSMarco: using an s-net like architecture with the answer string to predict response: pretrained on MSMarco.
\paragraph{Task-specific model for MSMarco}
The MSMarco task has the most opportunity for improvements, so we augment a good architecture for the task.

\paragraph{Pretraining}
For each task, we have too little data to learn an entire model, embeddings, so we focus on using the data to fine-tune the model to improve it's correlation. 
To learn the embeddings, we use pretrained models.

Note that we are able to use the labeled data to train our models as long as we use its prediction only on future data. 

% Simple linear/isotonic regression with simple language features like perplexity, n-gram overlap, etc., BLEU, etc.
\paragraph{Learning optimal scaling using ordinal regression}
Learn how to weigh above features to match ordinal / real-valued ratings.
