\section{Discussion}
\label{sec:discussion}

% Setting: shown how low correlations == bad
Prior work has shown that existing automatic metrics have poor instance-level correlation with mean human judgment and that they score many good quality responses poorly.
As a result, the evaluation is systematically biased against genuine system improvements that would lead to higher human evaluation scores but not improve automatic metrics.
% Key point: how do we make human evaluation more practical by decreasing the cost of doing 
In this paper, we have explored using an automatic metric to decrease the cost of human evaluation without introducing bias.
In practice, we find that with current automatic metrics and evaluation prompts data efficiencies are only 1.08--1.15 (7--13\% cost reduction).
Our theory shows that further improvements are only possible by improving the correlation of the automatic metric and reducing the annotator variance of the evaluation prompt.
As an example of how evaluation prompts could be improved, we found that using post-edits of summarizes decreased normalized annotator variance by a factor of three relative to using a Likert scale survey.
It should be noted that changing the evaluation prompt also changes the underlying ground truth $f(z)$: it is up to us to find a prompt that still captures the essence of what we want to measure.

% What other avenues are there to improve data efficiency?
%In conducting this work, we also explored several avenues to exploit automatic metrics to decrease estimation variance (costs).
% While key intuition for improving data efficiency lies in exploiting the automatic metric when it is reliable and falling back on human evaluation when it isn't, the challenges lies in knowing when the metric is reliable or not.
%Unfortunately, using a technique like importance sampling that is able to integrate uncertainties without introducing bias, but can significantly increase variance if the uncertainty estimates are not properly calibrated, as we found in preliminary experiments.

Without making stronger assumptions, the control variates estimator we proposed outlines the limitations of unbiased estimation.
Where do we go from here?
Certainly, we can try to improve the automatic metric (which is potentially as difficult as solving the task) and brainstorming alternative ways of soliciting evaluation (which has been less explored).
Alternatively, we could give up on measuring absolute scores, and seek instead to find techniques stably rank methods and thus improve them.
%As the NLP community tackles increasingly difficult tasks, human evaluation will only become more important.
%We hope our work provides some clarity on to how to make it more cost effective.

Finally, it is interesting to contrast the limitations on variance reductions we have proved in this chapter with the more encouraging results of \refchap{kbpo}.
While both methods guarantee unbiasedness, in \refchap{kbpo} we were able to exploit exact matches between the output of different systems to reuse annotations and hence amortize costs.
On the other hand, for the tasks studied in this chapter it is rare that two systems generate the exact same output: as a result the only way to share information between systems is through the automatic metric which we have seen to be a bottleneck in reducing variance.
Our findings apply not only to other text generation tasks such as machine translation or image captioning but in any setting with infinite incompleteness.
