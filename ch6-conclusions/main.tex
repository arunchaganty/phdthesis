\chapter[Conclusions]{\label{chap:discussion} Conclusions}

In the course of this thesis, we have presented several examples of an unmet assumption of the current evaluation paradigm for challenging information summarization tasks: that test collections are \textit{complete}, in that they contain the universe of all possible answers, and accurately represent the instances found in practice.
In \refchap{kbpo}, we showed how the finite incompleteness inherent in information extraction tasks, such as knowledge base population, introduces significant biases in our evaluation.
In \refchap{price}, we showed that when incompleteness is infinite, such as in text generation tasks, bias is pervasive and hard to eliminate.
Finally, in \refchap{otj}, we showed how incompleteness in the training data makes it hard to accurately evaluate or train systems without human intervention.

The crux of the solutions we provided in this thesis was combining on-demand human annotations with statistical techniques.
The human annotations, enabled by crowdsourcing platforms such at Amazon Mechanical Turk, allow us to effectively side-step the problem of incompleteness.
We have shown that using appropriate statistical techniques allows us to reduce the costs of collecting these annotations, sometimes by an order of magnitude.

In \refchap{kbpo}, we proposed importance-reweighted estimators for two widely used metrics, \textit{precision} and \textit{recall} and applied them to the task of knowledge base population.
The estimator overcomes limitations with existing importance-weighted estimators that allow it to apply previously collected annotations to new systems.
For both these metrics, we have been able to successfully reduce variance by a factor of 3--4 by amortizing over multiple systems in the finite incompleteness setting.
We also showed how the estimator can be integrated into an online evaluation service, providing not only unbiased evaluation scores, but also a quantitative error analysis.

In \refchap{price}, we identified an \textit{optimal} estimator to \textit{debias} automatic metrics like BLEU or ROUGE which are popularly used in tasks with infinite incompleteness like open-response question answering or text summarization.
We prove that the poor correlation between the automatic metrics and human judgment fundamentally lower bounds the number of annotations needed to correct the bias;
  in practice, we need almost as many human annotations to correct the bias as to conduct an complete human evaluation!
Our results shed light on how we can improve the evaluation procedures for such challenging tasks.

In \refchap{otj}, we use a statistical model to identify when it is uncertain about its own prediction and use that information to request crowdworkers for annotations \textit{at test time}. 
By casting the problem as a Bayesian decision problem, we are able to balance accuracy, cost and latency.
On three different classification tasks, we find an order of magnitude reduction in annotation relative to the human-only baseline, and significantly better accuracy relative to a model trained on a static dataset.

In the rest of this chapter, we will briefly discuss some challenges that face evaluation in natural language processing before concluding with a discussion of the role of evaluation in NLP.\@

\section{Challenges for evaluation}
In this section, we'll briefly review a few open challenges for the evaluation of complex tasks like information summarization as they relate to the work in this thesis.

\subsection{The cost and latency of evaluation}
While utilizing human feedback is currently necessary to evaluate the natural language processing tasks presented here,
  doing so incurs both costs to pay human annotators and latency.
In \refchap{otj}, we used ideas from game playing to strategically \textit{trade off} error rates, annotation cost and latency. 
Further improvements require new ideas.

One approach to decrease the costs of annotations is to \textit{gamify} the task to incentivize people to provide labels for their personal enjoyment.
For example, the ESP game~\citep{ahn2004labeling} was successfully able to collect thousands of image labels by asking two randomly paired people to try to label the image with the same word.
Gamification has also been applied to language tasks like word sense disambiguation~\citep{vannella2014validating} and coreference resolution~\citep{poesio2013phrase}.
For all it success, gamifying complex annotation tasks can be quite challenging, even more so if annotations are desired immediately.

The latency of acquiring human annotations can often be improved by parallelizing across multiple annotators, optimizing the annotation interface  or batching similar tasks to decrease worker context switching.
\citet{krishna2016embracing} combine all these ideas to show how images can be successfully labeled at human response times by tapping into the annotator's reflexes: once aggregated over several annotators, they are able to speed up annotation by a factor of 10 with only a small reduction in speed.

\subsection{Train-test mismatch}
The ultimate goal of quantitative evaluation is to get a reliable indicator for the performance of a system were it applied in practice.
Unfortunately, the machine learning community as a whole is trying to reconcile serious problems related to generalization even on classification tasks.
For example, even slight perturbations to the pixels of an image can wildly throw off a state-of-the-art image classification system that scores extremely well on \textit{the test set}~\citep{goodfellow2015explaining,carlini2016defensive,carlini2017adversarial}.
In fact, \citet{recht2018cifar} report a generalization gap even when the new test data is constructed in an identical (but independently collected) manner to the original training data!
Clearly, there is a problem in how we have been evaluating our systems and measuring progress thus far.
Test data that \textit{should} be representative of real-world instances does not seem to be.

In the field of natural language processing, too,
  there is a renewed discussion of how the performance of current NLP systems drops when applied to the real world~\citep{plank16nonstandard}:
for example, \citet{mcclosky2010any} show a 10--20 point gap between constituency parsing in domain and out of domain and \citet{foster2011news} find a similar gap on POS tagging and dependency parsing.
More recently, \citet{jia2017adversarial} find that even minor edits to the source text can throw off state of the art reading comprehension models.

The problem of resolving the mismatch between training sets and test sets has been extensively studied in the machine learning community.
Some popular routes to try and address this generalization problem include \textit{domain adaptation}~\citep{plank2011domain} (making the train and test data look more similar) and \textit{transfer learning}~\citep{weiss2016survey} (measuring the speed with which a system trained on a task A can learn to perform a task B).
As \citet{plank16nonstandard} argue, domain adaptation makes the unrealistic assumption that we know what the target domain is;
  on the other hand, transfer learning may allow us to developing better learning algorithms, it does little to tell us about the performance of a system on our desired task!

Another way to view the problem is one of incompleteness in the training data---it does not contain all the information necessary for the model to be able to generalize to the test set.
As a result, on-demand human annotation can also help resolve the mismatch between train-test sets.
We provide one possible solution in \refchap{otj} by using on-demand human annotations when the model is uncertain to guarantee high accuracy while minimizing annotation costs.

% \subsection{Evaluation of interactive systems}
% The on-demand evaluation framework we presented does not yet cover natural language tasks that need human interaction, e.g.\ dialogue systems~\citep{paek2007toward}.
% Here, the output of the system depends not only on the initial prompt, but also on every intervening response from the human and the system.
% As a result, two dialogue agents answering the same initial prompt may take different trajectories based on who they are interacting with.
% One of the biggest open questions is understanding what we should be evaluating in such settings and how to account for the variability introduced by the intermediate human responses.

\section{How should we evaluate evaluation?}
% (i) what's the goal of evaluation? there's the difference between evaluation for leaderboard numbers, evaluation for providing feedback to a human (error analysis), evaluation for providing feedback to a learning algorithm (this is a big one that we've talked about a lot)?
Before we conclude, let's take a step back to ask what the goals of evaluation are.
There is no doubt that having a robust, quantitative evaluation provides a clear direction for members in the research community to improve their systems and thus do science.
At the same time, we must be wary of oversimplifying what it means to solve a problem into a single number:
all too often these numbers hinge on over-fitting to dataset specific artifacts~\citep{gururangan2018annotation}.
While no evaluation is perfect, it is our opinion that we should strive for evaluation that provides us with \textit{actionable insights} that allow us to \textit{improve our systems}.
We hope that our work in \refchap{kbpo} provides a positive example of how annotation feedback can be used not only to provide a number to benchmark systems with, but also to provide error analysis and reveal new opportunities for research.

% (v) how important is the immediacy of getting an evaluation result?


% (iii) do you really need evaluation to be unbiased (related to the points above)?
% (iv) how does the bias problem of evalutaion relate to bias in machine learning in genreal?
Next, we would like to revisit the importance of \textit{unbiasedness} in evaluation.
In \refchap{kbpo} and \refchap{price}, we adhered to the statistical definition of unbiasedness: we required that our statistical estimates of performance always match human judgment for the same, irrespective of the system.
While this is an appealing guarantee for an evaluation methodology to have, we showed in \refchap{price} that it places fundamental limitations on how affordable we can make routine on-demand human evaluation.
It is likely that we will have to introduce appropriate inductive bias to make the evaluation of tasks such as open-response question answering or text summarization practical.
Unlike the systematic biases in the evaluation methodology we began with, carefully introducing inductive bias relies on assumptions that we can check and ensure are (mostly) met by the systems we evaluate.
% (ii) what are the prospects for automatic evaluation (BLEU, ROUGE)? be completely practical: are you actually advising people stop doing this or is there a place for it?  what about many forms of evaluation?
In contrast, we showed in \refchap{price} that automatic evaluation methods such as BLEU or ROUGE correlate so poorly with human judgment that they are not only weak indicators of performance, but are also easily gamed.
We strongly advise researchers to ensure that the automatic metrics used correlate with human judgment \textit{for the systems being evaluated} before using them.

In closing, we hope that this thesis has contributed to improving the state of evaluation in natural language processing.
Still, there is much work ahead to improve the evaluation of natural language processing systems:
  we must remain humble in recognizing the limitations of any evaluation.
As the techniques we seek to evaluate evolve, so too must our evaluation methodology.
