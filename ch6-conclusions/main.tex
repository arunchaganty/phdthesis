\chapter[Conclusions]{\label{chap:discussion} Conclusions}

In the course of this thesis, we have presented several examples of an unmet assumption of the current evaluation paradigm for challenging information summarization tasks: that test collections are \textit{complete}, in that they contain the universe of all possible answers, and accurately represent the instances found in practice.
In \refchap{kbpo}, we showed how the finite incompleteness inherent in information extraction tasks, such as knowledge base population, introduces significant biases in our evaluation.
In \refchap{price}, we showed that when incompleteness is infinite, such as in text generation tasks, bias is pervasive and hard to eliminate.
Finally, in \refchap{otj}, we showed how incompleteness in the training data makes it hard to accurately evaluate or train systems without human intervention.

The crux of the solutions we provided in this thesis was combining on-demand human annotations with statistical techniques.
The human annotations, enabled by crowdsourcing platforms such at Amazon Mechanical Turk, allow us to effectively side-step the problem of incompleteness.
We have shown that using appropriate statistical techniques allows us to reduce the costs of collecting these annotations, sometimes by an order of magnitude.

In \refchap{kbpo}, we proposed importance-reweighted estimators for two widely used metrics, \textit{precision} and \textit{recall} and applied them to the task of knowledge base population.
The estimator overcomes limitations with existing importance-weighted estimators that allow it to apply previously collected annotations to new systems.
For both these metrics, we have been able to successfully reduce variance by a factor of 3--4 by amortizing over multiple systems in the finite incompleteness setting.
We also showed how the estimator can be integrated into an online evaluation service, providing not only unbiased evaluation scores, but also a quantitative error analysis.

In \refchap{price}, we identified an \textit{optimal} estimator to \textit{debias} automatic metrics like BLEU or ROUGE which are popularly used in tasks with infinite incompleteness like open-response question answering or text summarization.
We prove that the poor correlation between the automatic metrics and human judgment fundamentally lower bounds the number of annotations needed to correct the bias;
  in practice, we need almost as many human annotations to correct the bias as to conduct an complete human evaluation!
Our results shed light on how we can improve the evaluation procedures for such challenging tasks.

In \refchap{otj}, we use a statistical model to identify when it is uncertain about its own prediction and use that information to request crowdworkers for annotations \textit{at test time}. 
By casting the problem as a Bayesian decision problem, we are able to balance accuracy, cost and latency.
On three different classification tasks, we find an order of magnitude reduction in annotation relative to the human-only baseline, and significantly better accuracy relative to a model trained on a static dataset.

In the rest of this chapter, we will briefly discuss some challenges that face evaluation in natural language processing before concluding with a discussion of the role of evaluation in NLP.\@

\section{Challenges for evaluation}
In this section, we'll briefly review a few open challenges for the evaluation of complex tasks like information summarization as they relate to the work in this thesis.

\subsection{The cost and latency of evaluation}
While utilizing human feedback is currently necessary to evaluate the natural language processing tasks presented here,
  doing so incurs both costs to pay human annotators and latency.
In \refchap{otj}, we used ideas from game playing to strategically \textit{trade off} error rates, annotation cost and latency. 
Further improvements require new ideas.

One approach to decrease the costs of annotations is to \textit{gamify} the task to incentivize people to provide labels for their personal enjoyment.
For example, the ESP game~\citep{ahn2004labeling} was successfully able to collect thousands of image labels by asking two randomly paired people to try to label the image with the same word.
Gamification has also been applied to language tasks like word sense disambiguation~\citep{vannella2014validating} and coreference resolution~\citep{poesio2013phrase}.
For all it success, gamifying complex annotation tasks can be quite challenging, even more so if annotations are desired immediately.

The latency of acquiring human annotations can often be improved by parallelizing across multiple annotators, optimizing the annotation interface  or batching similar tasks to decrease worker context switching.
\citet{krishna2016embracing} combine all these ideas to show how images can be successfully labeled at human response times by tapping into the annotator's reflexes: once aggregated over several annotators, they are able to speed up annotation by a factor of 10 with only a small reduction in speed.

\subsection{Train-test mismatch}
The ultimate goal of quantitative evaluation is to get a reliable indicator for the performance of a system were it applied in practice.
Unfortunately, the machine learning community as a whole is trying to reconcile serious problems related to generalization even on classification tasks.
For example, even slight perturbations to the pixels of an image can wildly throw off a state-of-the-art image classification system that scores extremely well on \textit{the test set}~\citep{goodfellow2015explaining,carlini2016defensive,carlini2017adversarial}.
In fact, \citet{recht2018cifar} report a generalization gap even when the new test data is constructed in an identical (but independently collected) manner to the original training data!
Clearly, there is a problem in how we have been evaluating our systems and measuring progress thus far.
Test data that \textit{should} be representative of real-world instances does not seem to be.

In the field of natural language processing, too,
  there is a renewed discussion of how the performance of current NLP systems drops when applied to the real world~\citep{plank16nonstandard}:
for example, \citet{mcclosky2010any} show a 10--20 point gap between constituency parsing in domain and out of domain and \citet{foster2011news} find a similar gap on POS tagging and dependency parsing.
More recently, \citet{jia2017adversarial} find that even minor edits to the source text can throw off state of the art reading comprehension models.

The problem of resolving the mismatch between training sets and test sets has been extensively studied in the machine learning community.
Some popular routes to try and address this generalization problem include \textit{domain adaptation}~\citep{plank2011domain} (making the train and test data look more similar) and \textit{transfer learning}~\citep{weiss2016survey} (measuring the speed with which a system trained on a task A can learn to perform a task B).
As \citet{plank16nonstandard} argue, domain adaptation makes the unrealistic assumption that we know what the target domain is;
  on the other hand, transfer learning may allow us to developing better learning algorithms, it does little to tell us about the performance of a system on our desired task!

Another way to view the problem is one of incompleteness in the training data---it does not contain all the information necessary for the model to be able to generalize to the test set.
As a result, on-demand human annotation can also help resolve the mismatch between train-test sets.
We provide one possible solution in \refchap{otj} by using on-demand human annotations when the model is uncertain to guarantee high accuracy while minimizing annotation costs.

% \subsection{Evaluation of interactive systems}
% The on-demand evaluation framework we presented does not yet cover natural language tasks that need human interaction, e.g.\ dialogue systems~\citep{paek2007toward}.
% Here, the output of the system depends not only on the initial prompt, but also on every intervening response from the human and the system.
% As a result, two dialogue agents answering the same initial prompt may take different trajectories based on who they are interacting with.
% One of the biggest open questions is understanding what we should be evaluating in such settings and how to account for the variability introduced by the intermediate human responses.

\subsection{Overcoming the limitations of unbiasedness}

% (v) how important is the immediacy of getting an evaluation result?
% (iii) do you really need evaluation to be unbiased (related to the points above)?
% (iv) how does the bias problem of evalutaion relate to bias in machine learning in genreal?
Next, we would like to revisit the importance of \textit{unbiasedness} in evaluation.
In \refchap{kbpo} and \refchap{price}, we adhered to the statistical definition of unbiasedness: we required that our statistical estimates of performance always match human judgment for the same, irrespective of the system.
While this is an appealing guarantee for an evaluation methodology to have, we showed in \refchap{price} that it places fundamental limitations on how affordable we can make routine on-demand human evaluation.
It is likely that we will have to introduce appropriate inductive bias to make the evaluation of tasks such as open-response question answering or text summarization practical.
Unlike the systematic biases in the evaluation methodology we began with, carefully introducing inductive bias relies on assumptions that we can check and ensure are (mostly) met by the systems we evaluate.
% (ii) what are the prospects for automatic evaluation (BLEU, ROUGE)? be completely practical: are you actually advising people stop doing this or is there a place for it?  what about many forms of evaluation?
In contrast, we showed in \refchap{price} that automatic evaluation methods such as BLEU or ROUGE correlate so poorly with human judgment that they are not only weak indicators of performance, but are also easily gamed.
We strongly advise researchers to prefer standardized human evaluations or to ensure that the automatic metrics used correlate with human judgment \textit{for the systems being evaluated} before using them.

\section{How should we evaluate evaluation?}
% (i) what's the goal of evaluation? there's the difference between evaluation for leaderboard numbers, evaluation for providing feedback to a human (error analysis), evaluation for providing feedback to a learning algorithm (this is a big one that we've talked about a lot)?
Before we conclude, let's take a step back to ask what the goals of evaluation are.
There is no doubt that having a robust, quantitative evaluation provides a clear direction for members in the research community to improve their systems and thus do science.
At the same time, we must be wary of oversimplifying what it means to solve a problem into a single number:
all too often these numbers hinge on over-fitting to dataset-specific artifacts~\citep{gururangan2018annotation} or biased evaluation metrics~\citep{chaganty2017unbiased}.
When our evaluation methodology is biased or does not correlate with human judgment, we run the same risks that the natural language processing community faced in the mid 1960s: 
to paraphrase John Pierce, \textit{we may end up building elaborate systems that either do very little or flop in an obscure ways. A lot of money and time will have been spent, but no simple, clear, sure knowledge would have been gained.}

We argue that there are two equally important objectives for evaluation: first, to benchmark systems and ensure ``simple, clear, sure knowledge'' is gained, we must have a \textit{fair} quantitative measure of performance.
Such a measure need not be perfectly correlated with human judgment, as long as it is not biased towards particular systems at the cost of others.
Ideally, the measure should be easy, quick and cheap to obtain so that researchers can iterate on systems or even integrate the metric into their training objectives.
In this work, we have shown that existing automatic metrics do not meet these criteria for complex tasks such as question answering or text summarization: though we have provided some solutions, there is much work yet to be done to reduce costs and latencies.

The second objective for evaluation is to provide the researcher with \textit{actionable insights} that allow her/him to improve the system and to verify that the model does not ``flop in obscure ways'':
  in addition to having interpretable models, we need to have \textit{interpretable evaluation}.
Some of the ways we can provide the researcher feedback is to better categorize and quantify the errors made by systems.
We believe that human feedback plays a critical role here, but so do statistical estimators that help measure the impact of various types of errors.
The work in this thesis has made progress on both of these fronts, showing how we can collect fine-grained feedback for relation extraction, entity linking, question answering and text summarization, and how this feedback can be quantified with unbiased estimates of their proportions.

In closing, we hope that this thesis has contributed to improving the state of evaluation in natural language processing.
Still, there is much work ahead to improve the evaluation of natural language processing systems:
  we must remain humble in recognizing the limitations of any evaluation.
As the techniques we seek to evaluate evolve, so too must our evaluation methodology.
