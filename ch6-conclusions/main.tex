\chapter[Towards a new paradigm of evaluation]{\label{chap:discussion} Towards a new paradigm of evaluation}

In the course of this thesis, we have presented several examples of an unmet assumption of the current evaluation paradigm: that test collections are \textit{complete}, in that they contain the universe of all possible answers, and accurately represent instances found in practice.
In \refchap{kbpo}, we showed how the finite incompleteness inherent in information extraction tasks, such as knowledge base population, introduces significant biases in our evaluation.
In \refchap{price}, we showed that when incompleteness is infinite, such as in text generation tasks, bias is pervasive and hard to eliminate.
Finally, in \refchap{otj}, we showed that even in complete settings such as (sequence) classification, the scarcity of annotated data makes it impossible to accurately evaluate or train systems without human intervention.
In this chapter, we will synthesize our findings on these different tasks to propose a new evaluation paradigm, on-demand evaluation, that overcomes these obstacles by asking for human feedback on system output.

More broadly, the machine learning community is trying to reconcile serious problems related to generalization even on classification tasks that recently surfaced~\citep{}.
For example, even slight perturbations to the pixels of an image can wildly throw off a state-of-the-art image classification system that scores extremely well on \textit{the test set}~\citep{}.
In fact, \citet{} report a generalization gap even when the new test data is constructed in an identical (but independently collected) manner to the original training data!
Clearly, there is a problem in how we have been evaluating our systems and measuring progress thus far:
  test data that \textit{should} be representative of real-world instances does not seem to be complete.

In the field of natural language processing (NLP),
\citet{plank16nonstandard} has begun a discussion how the performance of current NLP systems drops when applied to the real world: 
for example, \citet{mcclosky2010any} show a 10--20 point gap between constituency parsing in domain and out of domain and \citet{foster2011news} find a similar gap on POS tagging and dependency parsing.
Some popular routes to try and address the generalization problem include \textit{domain adaptation} (making the train and test data look more similar) and \textit{transfer learning} (measuring the speed with which a system trained on a task A can learn to perform a task B).
As \citet{plank16nonstandard} argue, domain adaptation makes the unrealistic assumption that we know what the target domain is;
  on the other hand, transfer learning may allow us to developing better learning algorithms, it does little to tell us about the performance of a system on our desired task!

Instead, we propose a new paradigm of on-demand evaluation that tries to evaluate systems on real data by collecting human feedback on system output as necessary.
Much of this chapter is dedicated to synthesizing the material of this thesis to describe a more general evaluation paradigm made possible by recent advances in crowdsourcing.
Our framework tries to address the aforementioned issues with \textit{evaluation} by making it more dynamic.
We close by discussing some limitations of the paradigm.

\section{The on-demand evaluation paradigm}

The key idea of the on-demand evaluation paradigm is to bring evaluation as close to our intended use case as possible, and to overcome data incompleteness or sparsity problems by querying human annotators on-demand.
\reffig{conclusions:overview} presents an overview of the framework: 
First, the evaluation designer ensures access to a suitable stream of \textit{input examples}
which are then processed by the system being evaluated.
The evaluation designer then reifies the various goals of evaluation by specifying a \textit{distribution} over system output and input examples.
Next, the evaluation framework identifies which examples require human annotations and queries humans on-demand for these annotations. 
Finally, the annotations obtained are combined with existing annotations to evaluate the system.

We'll describe the key design decisions and challenges for each of these steps next.

\paragraph{Input examples}
When identifying input examples, the evaluation designer must pick which documents to query the system on.
For example, in knowledge base population (\refchap{kbpo}), we used a large corpus of news-wire documents, which are a common set of input for the task.
Alternatively, we could have also included a webpage crawl or documents from social media.
Another source of inputs could be through actual user data (as we saw in \refchap{otj}).
Typically, which inputs to use is often well specified by the task.

\paragraph{Evaluation distribution}
In the current static test-collection driven evaluation, identifying input examples is often synonymous with picking which examples to annotate and evaluate on.
Typically the dataset designer (e.g., the LDC in the TAC KBP tasks) identifies examples that are ``interesting'', because they provide important or challenging use cases for the system.
Often, however, there are several aspects of our sysetms we would like to measure at once, e.g.\ performance on particular types of entities or relations in KBP or performance on certain genres of articles in text summarization.
Each of these goals would typically correspond to a separate set of evaluation queries. 

In the on-demand evaluation paradigm, we don't know which entities to query until we actually obtain output from the systems.
Thus, we must \textit{reify} our evaluation goals and identify specific query distributions that allow us to evaluate these.
Identifying the right query distribution can require some experimentation, as we document in \refchap{kbpo}.
However, as a bonus, having specific query distributions allows us to use the same set of samples to answer many different questions using the importance-reweighting method proposed in \refchap{kbpo}. 
Additionally, having a query distribution allows us to dynamically collect more data to evaluate a particular phenomenon in a system, as opposed to current test collections that may only contain a handful of examples for a particular phenomenon, making it impossible to evaluate its role.

The key challenge when identifying a query distribution is ensuring that it properly reflects our evaluation goal.
Given a query distribution, it is usually straightforward to sample from it.
In \refchap{otj} we used the system's confidence scores to dynamically decide what to annotate.
Unfortunately, it is hard to do so with guarantees because the distribution over samples changes as the model changes. 

\paragraph{Human intervention}
Once we have identified some examples to annotate, we must query human annotators for their feedback.
The precise interface to do so varies depending on the task and can range from something very simple like a Likert scale survey in \refchap{price} to something significantly more complex like the exhaustive annotation interfaces presented in \refchap{kbpo}.

Another consideration when deciding which annotations to solicit from people is whether the annotations are \textit{interpretable}.
We have found that asking annotators for explanations not only increases the quality of the annotation, but also makes it interpretable.

\paragraph{Statistical estimation}
Finally, we must aggregate the human annotations to provide a single score. 
The statistical estimator is at the heart of this paradigm.
The key contributions of this thesis is to show how we can combine different measures in a sound way.
One of the most important challenges and contributions is identifying recall.

Shown how to do it with overlapping sets.
Shown how to do it with automatic metrics.

Direct extensions are to identity sets, needed by entity linking.

\subparagraph{Amortization}

In order for the paradigm to succeed, we need to ensure there is some amortization: are we able to reuse the annotations collected for one system on another?
Successfully show this to be the case in KBPO\@.
Unfortunately, we do not yet know how to do this with language generation. One approach may be to collect new references.

\subparagraph{Error analysis}

Another advantage of this approach, is with interpretable feedback from people (who get to view the system output when annotating) is the ability to analyze the errors they make and provide a statistically sound automated error analysis.


\section{Challenges}

\paragraph{Input selection}
\paragraph{Amortization}
\paragraph{Human annotation}
\paragraph{Automating error analysis}

\section{Limitations}

\paragraph{Cost and latency.}
Some progress with more efficient amortization.
Can not work around the fact that querying humans takes time. In OTJ, we have shown that we can reduce the time significantly (trading off on cost) on some examples.

\paragraph{Dynamic evaluation.}
Some evaluation settings may need human intervention to help systems to even generate output, e.g.\ chatbots.
The methods presented here do not extend well in this setting because statistics is hard.

\section{Conclusions}
There is much work ahead to make this a reality.

We believe this thesis has set the key pillars and framework.

Evaluation is important, hope to have contributed.

