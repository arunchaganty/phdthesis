\chapter[Towards a new paradigm of evaluation]{\label{chap:discussion} Towards a new paradigm of evaluation}

In the course of this thesis, we have presented several examples of an unmet assumption of the current evaluation paradigm: that test collections are \textit{complete}, in that they contain the universe of all possible answers, and accurately represent the instances found in practice.
In \refchap{kbpo}, we showed how the finite incompleteness inherent in information extraction tasks, such as knowledge base population, introduces significant biases in our evaluation.
In \refchap{price}, we showed that when incompleteness is infinite, such as in text generation tasks, bias is pervasive and hard to eliminate.
Finally, in \refchap{otj}, we showed that even in complete settings such as (sequence) classification, the scarcity of annotated data makes it impossible to accurately evaluate or train systems without human intervention.
In this chapter, we will synthesize our findings on these different tasks to propose a new evaluation paradigm, on-demand evaluation, that overcomes these obstacles by asking for human feedback on system output.

More broadly, the machine learning community is trying to reconcile serious problems related to generalization even on classification tasks.
For example, even slight perturbations to the pixels of an image can wildly throw off a state-of-the-art image classification system that scores extremely well on \textit{the test set}~\citep{goodfellow2015explaining,carlini2016defensive,carlini2017adversarial}.
In fact, \citet{recht2018cifar} report a generalization gap even when the new test data is constructed in an identical (but independently collected) manner to the original training data!
Clearly, there is a problem in how we have been evaluating our systems and measuring progress thus far.
Test data that \textit{should} be representative of real-world instances does not seem to be complete.

In natural language processing (NLP) field,
  there is a renewed discussion of how the performance of current NLP systems drops when applied to the real world~\citet{plank16nonstandard}: 
for example, \citet{mcclosky2010any} show a 10--20 point gap between constituency parsing in domain and out of domain and \citet{foster2011news} find a similar gap on POS tagging and dependency parsing.
More recently, \citet{jia2017adversarial} find that even minor edits to the source text can throw off state of the art reading comprehension models.

Some popular routes to try and address the generalization problem include \textit{domain adaptation} (making the train and test data look more similar) and \textit{transfer learning} (measuring the speed with which a system trained on a task A can learn to perform a task B).
As \citet{plank16nonstandard} argue, domain adaptation makes the unrealistic assumption that we know what the target domain is;
  on the other hand, transfer learning may allow us to developing better learning algorithms, it does little to tell us about the performance of a system on our desired task!

Instead, we propose the new paradigm of on-demand evaluation that tries to evaluate systems on real data by collecting human feedback on system output as necessary.
Much of this chapter is dedicated to synthesizing the material of this thesis to describe a more general evaluation paradigm made possible by recent advances in crowdsourcing.
Our framework tries to address the aforementioned problems with evaluation by making it more dynamic.
We close by discussing some limitations of the paradigm.

\section{The on-demand evaluation paradigm}

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{figures/overview}
  \caption[Overview of the on-demand evaluation paradigm]{\label{fig:conclusions:overview}
  An overview of the on-demand evaluation paradigm. 
  First, we must identify a \textit{source of inputs} (e.g.\ documents) or a distribution over the same that are provided to the system.
  Next, we specify which inputs to evaluate using a \textit{query distribution}; the query distribution plays the role of a dataset designer by identifying which system output we would like to prioritize.
  The lynchpin of the paradigm is conducting a \textit{human annotation} as necessary on the system output. 
  Finally, annotations from different systems are combined using a \textit{statistical estimator} to produce a final evaluation score.
  }
\end{figure}

The key idea of the on-demand evaluation paradigm is to bring evaluation as close to our intended use case as possible, and to overcome data incompleteness or sparsity problems by querying human annotators on-demand.
\reffig{conclusions:overview} presents an overview of the framework: 
First, the evaluation designer ensures access to a suitable stream of \textit{input examples}
which are then processed by the system being evaluated.
The evaluation designer then reifies the various goals of evaluation by specifying a \textit{distribution} over system output and input examples.
Next, the evaluation framework identifies which examples require human annotations and queries humans on-demand for these annotations. 
Finally, the annotations obtained are combined with existing annotations to evaluate the system.

We'll describe the key design decisions and challenges for each of these steps next.

\subsection{Input source}
When identifying input examples, the evaluation designer must pick which documents to query the system on.
For example, in knowledge base population (\refchap{kbpo}), we used a large corpus of news-wire documents, which is reflective of the documents that the system would be used with in practice.
Alternatively, we could have also included a webpage crawl or documents from social media.
Another source of inputs could be actual user data (as we saw in \refchap{otj}).
Typically, the source of inputs is governed by the task.

\subsection{Query distributions}
In the current static test-collection driven evaluation, identifying input examples is often synonymous with picking which examples to annotate and evaluate on.
Typically the dataset designer (e.g., the LDC in the TAC KBP tasks) identifies examples that are ``interesting'', because they provide important or challenging use cases for the system.
Often, however, there are several aspects of our systems we would like to measure at once, e.g.\ performance on particular types of entities or relations in KBP or performance on certain genres of articles in text summarization.
Each of these goals would earlier correspond to a separate set of evaluation queries. 

In the on-demand evaluation paradigm, 
  we acknowledge that it is sometimes hard to know exactly which examples should be evaluated until we obtain output from systems.
Thus, we must \textit{reify} what the goals of our evaluation are and identify specific query distributions that formalize them.
\refchap{kbpo} we saw a couple of examples of these: one query distribution could target performance of the system on particular classes, while another could focus on rare entities.
Unlike in the earlier mode of creating several separate test collections, in \refchap{kbpo} we show how we can use the same set of annotated examples to evaluate many different query distributions using importance-reweighting.

The key challenge when identifying a query distribution is ensuring that it properly reflects our evaluation goal.
Identifying the right query distribution does require some experimentation, but also allows us to draw more precise and statistically significant conclusions.
Given a static query distribution, it is usually straightforward to sample from it.
That said, it is also possible to dynamically update our query distribution during our evaluation.
In \refchap{otj} we used the system's confidence scores to dynamically decide what to annotate.
Unfortunately, an open question is what statistical guarantees we can obtain when dynamically identifying samples.

\subsection{Human annotations}
Once we have identified which examples to annotate, we must query human annotators for their feedback.
The ability to get human annotations on demand is the lynchpin of this new paradigm and is enabled by a host of modern crowdsourcing frameworks like \href{requester.mturk.com}{Amazon Mechanical Turk}, \href{https://www.figure-eight.com/}{Figure Eight} or \href{https://daemo.org}{Daemo}~\cite{gaikwad2015daemo}.
These platforms serve as labor marketplaces where it is easy and quick to hire people to answer questions, label images or text, etc.

\paragraph{Annotation interfaces.}
Having access to human annotators is particularly well suited for our purposes because the underlying difficulty when evaluating tasks such as text summarization, question answering or knowledge base population is human subjectivity.
Put differently, if our end goal is to provide summarize information \textit{for people}, it is fitting that it be judged by people as well.
At the same time, it is often non-trivial to properly communicate the subjective evaluation criteria to a non-expert annotator.
Thus, interface design and instructions are an incredibly important components when crowdsourcing.
The precise interface varies depending on the task and can range from something very simple like a Likert scale survey in \refchap{price} to something significantly more complex like the exhaustive annotation interfaces presented in \refchap{kbpo}.

Depending on the task, it may also be necessary to reformulate what annotations we obtain.
In \refchap{price}, we show that asking people to \textit{edit} a system generated summary and measuring the edit distance results in a more objective metric than simply asking for Likert scale judgments.
When annotating a linguistically nuanced task like semantic role labeling, \citet{he2015question} simplified the annotation schema by using specific questions for roles, e.g., asking ``who finished something'' in order to identify the ``agent''.

\paragraph{Interpretable annotations.}
Another consideration when deciding which annotations to solicit from crowdworkers is whether or not the annotations are \textit{interpretable}.
Interpretable annotations like the edits or highlighting justifications we collected in \refchap{price} not only make it easier to identify systematic annotation errors (which can be resolved e.g.\ by iterating on the instructions) and make the task easier to understand for the crowd worker, but also enable finer grained quantitative error analysis.
As another example, \citet{ling2017teaching} showed how natural language feedback could be used to learn how to caption images better.

\paragraph{Correcting errors in annotations.}
Despite these different approaches to improving the quality of annotations, it is hard to avoid errors made by confused or malicious workers.
Explicit models of the workers and their errors, e.g., ones proposed by \citet{dawid1979maximum}, \citet{passonneau2014benefits} or \citet{branson2017lean}, allow us to aggregate information across multiple workers to correct their errors.
In \refchap{otj}, we showed how we could use a model to dynamically identify when crowdworkers may be confused and used this information to collect additional annotations.
Such models will be necessary when evaluating complex subjective tasks like text summarization.

\subsection{Statistical estimation}
Finally, we must aggregate the human annotations to provide a meaningful quantitative score for a system.
Naively, we can always simply aggregate a suitable metric like precision, accuracy or edit distance using the human annotations, but this will not be economically scalable in the long run.
Thus, a statistical estimator that allows us to \textit{amortize} costs as we evaluate multiple systems \textit{and multiple evaluation metrics} is crucial to making the on-demand evaluation paradigm practical.
The key contribution of this thesis is showing that we can amortize costs with the appropriate statistical estimation techniques.

\paragraph{Precision and recall.}
In \refchap{kbpo}, we proposed efficient estimators for two widely used metrics, \textit{precision} and \textit{recall}.
Computing precision relies entirely on the output generated by a single system, but we showed that by using the annotations from other systems we were able to decrease the variance by a factor of four.

Accurately measuring and improving on \textit{true recall} is vital to being able to develop systems that identify \textit{new} things.
Thus far, most evaluations of open-domain tasks like knowledge base population have only reported \textit{pooled recall}, an approximation that relies on the predictions of a static collection of systems.
We showed that this approach results in a significant and systematic bias that prevents researchers from being able to measure genuine improvements.  
Instead, we leverage the recall calculated on a small, randomly selected collection of documents from the corpus.
Using our statistical estimator, we are able to correct the bias of pooled recall computed on a growing set of systems and thus decrease variance by a factor of almost four.

In both these cases, we have been able to successfully amortize costs over multiple participating systems.
The degree of cost saving really depends on the extent of overlap between systems, is best suited for problems with finite incompleteness.
We believe these techniques should easily transfer to clustering based metrics needed for entity detection and linking and co-reference resolution.

\paragraph{Black-box metrics.}
In \refchap{price}, we execute the on-demand evaluation paradigm in the setting of \textit{infinite incompleteness}, where no two systems are likely to significantly overlap in their predictions.
Of course, as a consequence, system answers also do not overlap with the reference answers in test collections.
The existing solution to this problem is to adopt a \textit{automatic metric} that serves as a similarity function between the system generated answer and reference answer.
Unfortunately, we show that existing automatic metrics not only correlate poorly with human judgment, but are \textit{biased} in that their correlations differ significantly based on the system.

In our work, we have identified an \textit{optimal} estimator to \textit{debias} these automatic metrics.
We prove that the poor correlation also fundamentally lower bounds the number of annotations need to correct the bias;
  in practice, we need almost as many human annotations to correct the bias as to conduct an complete human evaluation!
Thus, in the setting of infinite incompleteness, we have provided optimal estimation techniques, but still haven't been able to successfully amortize costs.

\paragraph{Active learning and dynamic sampling.}
Hypothetically, we could make annotation more efficient if we could dynamically pick which samples to annotate based on the systems performance on previously collected samples.
The key challenge in doing so is correcting for the sampling bias that arises as the distribution over what to sample changes every time we perform an annotation.
Unfortunately, much of the active learning literature focuses only on the generalization performance of a model trained on the samples, and not on unbiased estimation.
The dynamic annotation scheme we proposed in \refchap{otj} is able to evaluate and error-correct systems \textit{at test time}, but does not have access to a query distribution on which to compute an evaluation score.
As such, identifying an efficient, unbiased, dynamic annotation scheme is an important open problem for on-demand evaluation.

\paragraph{Quantitative error analysis.}
Apart from merely obtaining a single quantitative score with which to compare systems, the goal of evaluation is to quantify the limitations of current systems in order to provide direction for future work.
This is often manifested in authors conducting a qualitative error analysis, wherein a small sample of system output is manually inspected and coded for particular language errors.
With humans-in-the-loop, it is possible to collect far more data and conduct a more fine-grained error analysis than was previously possible.

However, by using an appropriate statistical estimator, we can also significantly decrease the costs of conduct fine-grained error analysis by efficiently reusing annotations.
As an example, in \refchap{kbpo} we show how we can accurately analyze the errors made by a knowledge base population system on different relations and entity linking without having to re-annotate any data.

\section{Limitations}
Having presented an overview of the on-demand evaluation framework, let's now look at some of its limitations.

\paragraph{Cost and latency.}
While utilizing human feedback is currently necessary to evaluate the natural language processing tasks presented here,
  doing so incurs both costs to pay human annotators and latency.
In \refchap{otj}, we used ideas from game playing to strategically \textit{trade off} error rates, annotation cost and latency. 
Further improvements require new ideas.

One approach to decrease the costs of annotations is to \textit{gamify} the task to incentivize people to provide labels for their personal enjoyment.
For example, the ESP game~\citep{ahn2004labeling} was successfully able to collect thousands of image labels by asking two randomly paired people to try to label the image with the same word.
Gamification has also been applied to language tasks like word sense disambiguation~\citep{vannella2014validating} and coreference resolution~\citep{poesio2013phrase}.
For all it success, gamifying complex annotation tasks can be quite challenging, even more so if annotations are desired immediately.

The latency of acquiring human annotations can often be improved by parallelizing across multiple annotators, optimizing the annotation interface  or batching similar tasks to decrease worker context switching.
\citet{krishna2016embracing} combine all these ideas to show how images can be successfully labeled at human response times by tapping into the annotator's reflexes: once aggregated over several annotators, they are able to speed up annotation by a factor of 10 with only a small reduction in speed.

\paragraph{Integration with training.}
In this thesis, we have seen how we can properly evaluate complex natural language tasks with human feedback.
Could we use the same human-in-the-loop paradigm to \textit{train} machine-learning based systems for these tasks?
In \refchap{otj} we showed one approach that was able to use human feedback to efficiently and cost-effectively train relatively simple models.
However, many modern natural language processing systems have millions of parameters and require tens to hundreds of thousands of labeled examples:
  the cost and latency of utilizing humans-in-the-loop presents a serious bottleneck during training loop.

One approach to integrate human feedback during training is to learn how to simulate human feedback (i.e.\ learn a ``reward function'').
For example, \citet{christiano2017deep} collected human preferences on robot trajectories and found that by first learning a model of human feedback, they were able to speed up the training of the robot.
It is yet to be seen how such ideas can be extended to language tasks.

\paragraph{Dynamic evaluation.}
Finally, the evaluation framework we presented does not yet cover natural language tasks that need human interaction, e.g.\ chatbots.
Here, the output of the system depends not only on the initial prompt, but also on every intervening response from the human and the system.
As a result, two chatbot systems answering the same initial prompt may take different trajectories based on who they are interacting with.
One of the biggest open questions is understanding what we should be evaluating in such settings and how to account for the variability introduced by the intermediate human responses.

\section{Conclusions}
In this thesis, we have introduced a new paradigm of evaluation, on-demand evaluation, that enables evaluation on complex subjective  tasks that can not be adequately captured through test collections.
We exploit advances in crowdsourcing to collect human feedback to evaluate systems as necessary and develop new statistical estimators to make efficient use of the feedback.
We have executed the on-demand evaluation paradigm on several challenging use cases, from knowledge base population to text summarization.

Still, there is much work ahead to improve the evaluation of natural language processing systems.
We hope that the discussion in this last chapter has provided some direction on where to go forward.
However, we must remain humble in recognizing the limitations of any evaluation.
As the techniques we seek to evaluate evolve, so too must our evaluation methodology.
