\section{On-demand evaluation for KBP}
\label{sec:kbpo:application}
Applying the on-demand evaluation framework to a task requires us to answer three questions:
\begin{enumerate}
    \itemsep0pt
  \item What is the desired distribution over system predictions $p_i$?
  \item How do we label an instance $x$, i.e.\ check if $x \in \sY$?
  \item How do we sample from the unknown set of true instances $x \sim p_0$?
\end{enumerate}
In this section, we present practical implementations for knowledge base population.

\subsection{Sampling from system predictions}
% 1. what is the point of this distribution?
% - prevent over sampling instances.
%\ac{@PL:\@ I think that the role of $p_i$ is really very subtle, but very important. I hope the below paragraph highlights the paradigm shift in how one has to think about on-demand evaluation.}
Both the official TAC-KBP evaluation and the on-demand evaluation we propose use micro-averaged precision and recall as metrics. However, in the official evaluation, these metrics are computed over a fixed set of evaluation entities chosen by LDC annotators, resulting in two problems: (a) defining evaluation entities requires human intervention and (b) typically a large source of variability in evaluation scores comes from not having enough evaluation entities (see e.g.\ \citep{webber2010measurement}). In our methodology, we replace manually chosen evaluation entities by sampling entities from each systemâ€™s output according $p_i$. In effect, $p_i$ makes explicit the decision process of the annotator who chooses evaluation entities.

%\ac{@PL:\@ This paragraph goes into details, but I felt that it was useful to illustrate how we tailored a distribution to meet our evaluation goals.}
Identifying a reasonable distribution $p_i$ is an important implementation decision that depends on what one wishes to evaluate.
Our goal for the on-demand evaluation service we have implemented is to ensure that KBP systems are fairly evaluated on diverse subjects and predicates, while at the same time, ensuring that entities with multiple relations are represented to measure completeness of knowledge base entries.
The dual objectives of balancing the head and tail of the distribution apply broadly other tasks as well, such as information retrieval.

To pick the specific distributions to use in our work,
we looked to the evaluation guidelines of the TAC KBP competition developed by the Linguistic Development Consortium (LDC)~\citep{ellis2015tackbp,mayfield2012evaluating} as a well-respected community standard and tried to adapt it to our framework.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/analysis/distribution}
  \caption[TAC KBP 2015 Query entity distribution]{\label{fig:kbpo:distribution}
    The solid line plots a histogram of how many documents a particular entity appeared in the TAC KBP 2016 corpus.
    The distribution is approximately a power-law distribution.
    Overlayed is a histogram of the frequency of the actual query entities used in the TAC KBP 2015 evaluation, binned by their frequency. We consider entities that appear in 3 or less unique documents (the 50th percentile) to be ``low'' frequency entities, those that appear more than 154 unique document (90th percentile) to be ``high'' frequency entities and those that appear in between to be ``medium'' frequency entities.
    The TAC KBP evaluation prefers medium frequency and low frequency entities.
  }
\end{figure}

\paragraph{The LDC query distribution and metrics.}
The first aspect we looked into was the distribution of entities that were queried in the official evaluations. 
The KBP evaluation is well known for using query entities that participate in several relations but are still not common enough to have a knowledge base entry on say Wikipedia.
We wanted to capture this property in our evaluation as well.

To get a better sense of the distribution of entities that were queried, we used the entities (i.e., people and organizations) that were recognized by the Stanford KBP entity linking system~\citep{stanford2017kbp} and plotted a log-log distribution of the number of entities that appear in some number of documents (\reffig{kbpo:distribution}).
As one might expect, the distribution approximately follows a power-law and is extremely long-tailed: there are many entities that appear in just one or two documents. 

We then binned entities into three categories, ``low'' frequency entities that appear in at most 3 documents in the corpus (this is the 50th percentile), ``medium'' frequency entities that appear in between 3 and 154 (90th percentile) documents and finally, ``high'' frequency entities that appear in more than 154 documents.
The proportion of query entities in these three bins has been overlayed on top of the distribution in \reffig{kbpo:distribution}.
We see that a majority of the entities queried are medium frequency, followed by low frequency entities and a handful of high frequency entities.
We would like to ensure that our sampling distributions adequately capture these medium frequency entities. 

 \begin{figure}
   \centering
   \begin{subfigure}{0.7\textwidth}
     \includegraphics[width=\textwidth]{figures/analysis/selective_supervised_entity}
     \caption{\label{fig:kbpo:selective-supervised-entity} Distribution over entity classes}
   \end{subfigure}
 
   \begin{subfigure}{0.7\textwidth}
     \includegraphics[width=\textwidth]{figures/analysis/selective_supervised_clusters}
     \caption{\label{fig:kbpo:selective-supervised-clusters} Distribution over entity clusters}
   \end{subfigure}
 
   \caption[Comparison of relation sampling schemes on their entity distributions]{\label{fig:kbpo:selective-supervised}
   We compared the types of entities that different relation instance sampling schemes preferred when we sampled 1,000 instances using them.
   Plots are averaged over 100 draws.
   (a) In terms of low, medium and high frequency entities, both $p^{\text{(subj)}}$ and $p^{\text{(subj-reln)}}$ sample fewer high frequency entities which are otherwise over represented.
   (b) In terms of how many entities were sampled with more than one relation (a cluster) were sampled, $p^{\text{(subj-reln)}}$ is still able to capture a comparable number of these unlike $p^{\text{(reln)}}$.
   }
 \end{figure}
 
 \begin{figure}
   \centering
   \includegraphics[width=0.9\textheight, angle=-90]{figures/analysis/selective_supervised_relations}
   \caption[Comparison of relation sampling schemes on their relation distributions]{\label{fig:kbpo:selective-supervised-relation}
   We compared the types of relations that different relation instance sampling schemes preferred when we sampled 1,000 instances using them.
   Plots are averaged over 100 draws.
   Both $p^{\text{(inst)}}$ and $p^{\text{(subj)}}$ do not draw any instances for particular relations, e.g.\ \texttt{org:shareholders}, while
   $p^{\text{(rel)}}$ and $p^{\text{(subj-rel)}}$ adequately represent each relation.
   }
 \end{figure}
 
 \paragraph{Identifying diverse clustered relations.}
 Finally, we wanted to ensure that the entities we sampled from systems were well distributed both over medium frequency entities and over relations.
 Unsurprisingly, the naive approach of uniformly sampling relations outputted by systems, is dominated by  high frequency entities (\reffig{kbpo:selective-supervised-entity}) on a few common relations (e.g., \texttt{per:title}; see \reffig{kbpo:selective-supervised-relation}).
 
 Our first attempt at rectifying the problem was to use two distributions that sampled relations inversely proportional to the frequency of their subject entity and relation label respectively:
 \begin{align*}
   p^{\text{(subj)}}(x) &\propto \frac{1}{|\texttt{subj}(x)|} &
   p^{\text{(rel)}}(x) &\propto \frac{1}{|\texttt{rel}(x)|}
 \end{align*}
 These distributions solved the two problems individually, but could not be combined because they differed in their support.
 There was an additional problem: we hardly ever selected two relations from the same entity, as shown in \reffig{kbpo:selective-supervised-clusters}.
 We wanted to maintain this property to be able to test the entity linking component of systems.
 
 Our final proposed distribution rectified this by combining $p^{\text{(subj)}}(x)$ and $p^{\text{(rel)}}(x)$ and including a factor for the number of \textit{other} relations the subject entity had.
 \begin{align*}
   p^{\text{(subj-rel)}}(x) &\propto \frac{|\texttt{subj-relns}(x)|}{|\texttt{subj}(x)| |\texttt{rel}(x)|},
 \end{align*}
 \reffig{kbpo:selective-supervised-entity} shows how this new distribution has both a better representation of medium frequency entities and is well balanced across all the relations.
 \reffig{kbpo:selective-supervised-clusters} also shows that it is able to represent clusters of relations with the same entity.

\begin{figure}[!th]
  \centering
  \begin{subfigure}{\textwidth}
  \centering
    \includegraphics[width=0.9\textwidth]{figures/interface/relation-interface}
    \caption{\label{fig:kbpo:relation-interface}}
  \end{subfigure}

  \begin{subfigure}{\textwidth}
  \centering
    \includegraphics[width=0.9\textwidth]{figures/interface/extraction-interface}
    \caption{\label{fig:kbpo:entity-interface}}
  \end{subfigure}

  \caption[Annotation interfaces for KBP]{\label{fig:kbpo:interfaces}
  Interfaces for annotating (a) relations and (b) entities.
  }
\end{figure}

\subsection{Labeling predicted instances}
We label predicted relation instances by presenting the instance's provenance to crowdworkers
  and asking them to identify if a relation holds between the identified subject and object mentions (\reffig{kbpo:relation-interface}). 
  Crowdworkers are also asked to link the subject and object mentions to their canonical mentions within the document and to pages on Wikipedia, if possible, for entity linking.
On average, we find that crowdworkers are able to perform this task in about 20 seconds, corresponding to about \$0.05 per instance.
We requested 5 crowdworkers to annotate a small set of 200 relation instances from the 2015 TAC-KBP corpus 
and measured a substantial inter-annotator agreement with a Fleiss' kappa of 0.61 with 3 crowdworkers and 0.62 with 5. % \pl{why does it go up with more annotators?}.
%On manually inspection of these instances, we observed a precision of about 75\%.
%While this precision is slightly less than the 80\% obtained by annotators at LDC \citep{ellis2016overview}, we believe it could be easily improved with appropriate changes to the annotation interface.
Consequently, we take a majority vote over 3 workers in subsequent experiments.
%leading to a total cost of \$0.15 per relation instance.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/analysis/exhaustive_entity_cross}
  \caption[Comparison of document sampling distributions]{\label{fig:kbpo:exhaustive-entity}
  In order to properly test the entity linker when measuring recall, an important feature of KBP systems, we must identify documents for exhaustive annotation that contain some entity clusters.
  A sample of 200 documents uniformly picked from the document collection only exhibits clusters for high frequency entities.
  Our TF-IDF sampling scheme increases the frequency with which medium and low frequency entities appear across multiple documents in the sample.
  }
\end{figure}

\subsection{Sampling true instances}
Sampling from the set of true instances $\sY$ is difficult because we can't even enumerate the elements of $\sY$.
As a proxy, we assume that relations are identically distributed across documents and have crowdworkers annotate a random subset of documents for relations.

Here too, we wanted to come up with a good distribution with which to query documents to exhaustively annotate.
 It is not enough for us to ensure the documents contain a good distribution of medium frequency entities: we must also ensure that these entities appear in multiple documents to fairly test systems' entity linking components.
 
 First, we tried using documents that were sampled uniformly from the corpus.
 As \reffig{kbpo:exhaustive-entity} shows, 
 a randomly sampled document contains a good range of low, medium and high frequency entities, but almost none of these entities are shared across the documents of the collection.
 We note that the fact that low frequency entities dominate the entity distribution is to be expected because they constitute the majority of entities in any given document,\footnote{%
   We also tried sampling documents based on the frequency of entities they contained (estimated using the Stanford entity linker), but found that it did not significantly reduce the number of low frequency entities.} but we can correct for this when sampling relations.
 
 Instead, we used a two-step sampling procedure.
 First, we randomly sample documents for 20\% of our exhaustive document collection and then sample the remaining documents proportional to their aggregate TF-IDF scores with the entities identified in the first sample.
 \reffig{kbpo:exhaustive-entity} shows the distribution of entities that were sampled by this method, using the Stanford entity linking system to identify entities in the first sample.
 The method is able to increase the number of medium frequency entities appearing in multiple documents.
 When implementing this method in practice, we explicitly avoided using the Stanford entity linking system when identifying entities in documents because we were concerned about the bias it would introduce into our recall estimates.
 Instead, we identified the entities in the first sample using crowdsourcing with our exhaustive annotation interface.

\subsection{Labeling true instances}
Once a random subset of documents was selected, we asked crowdworkers to identify relations using an interface we developed (\reffig{kbpo:entity-interface}).
Crowdworkers begin by identifying every mention span in a document.
  For each mention, they are asked to identify its type, canonical mention within the document
  and associated Wikipedia page if possible.
They are then presented with a separate interface to label predicates between pairs of mentions within a sentence that were identified earlier.

We compare crowdsourced annotations against those of expert annotators using data from the TAC KBP 2015 EDL task on 10 randomly chosen documents.
We find that 3 crowdworkers together identify 92\% of the entity spans identified by expert annotators, while 7 crowdworkers together identify 96\%.
When using a token-level majority vote to identify entities, 3 crowdworkers identify about 78\% of the entity spans; this number does not change significantly with additional crowdworkers.
We also measure substantial token-level inter-annotator agreement using Fleiss' kappa for identifying typed mention spans ($\kappa = 0.83$), canonical mentions ($\kappa = 0.75$) and entity links ($\kappa = 0.75$) with just three workers.
Based on this analysis, we use token-level majority over 3 workers in subsequent experiments.
%We also manually labeled 200 relation instances that from the exhaustively annotated documents and observed a precision of about 75\%.
%While this precision is slightly less than the 80\% obtained by annotators at LDC \citep{ellis2016overview}, we believe it could be easily improved with appropriate changes to the annotation interface.

The entity annotation interface is far more involved and takes on average about 13 minutes per document, corresponding to about \$2.60 per document, while the relation annotation interface takes on average about \$2.25 per document.
Because documents vary significantly in length and complexity, we set rewards for each document based on the number of tokens (.75c per token) and mention pairs (5c per pair) respectively.
With 3 workers per document, we paid about \$15 per document on average.
Each document contained an average 9.2 relations, resulting in a cost of about \$1.61 per relation instance.
We note that this is about ten times as much as labeling a relation instance.

%We defer details regarding how documents themselves should be weighted to capture diverse entities that span documents to \refapp{implementation}.
%We provide details regarding our sampling scheme and its distribution over entities in \refapp{implementation} of the supplementary material.
%When considering uniformly sampled documents, we found that a majority of the relations extracted correspond to very rare entities and result in very few entities with more than one relation (\reffig{entity-distribution}).
%In contrast, the TAC KBP query are almost evenly split between rare and semi-frequent entities.
%As a heuristic, we adopt the following two-stage sampling procedure:
%First, 20\% of our exhaustive document collection is sampled uniformly and annotated.
%We then uniformly sample the entities annotated to create a collection of ``evaluation entities''.
%Finally, we construct the remaining 80\% of our document collection by searching for documents that contain the evaluation entities according to an exact string match. This process results in far more entities of medium frequency.
