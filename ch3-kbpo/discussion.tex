\section{Related work}
\label{sec:kbpo:related}

%Knowledge base population bears several similarities with information retrieval: we are also interested in answering a specific information need, a relation, from a large document corpus.
%It follows that research into information retrieval evaluation is extremely relevant.
% Axes of comparison:
% - identifying and measuring pooling bias
% Origin of pooling: K. Sp¨ arck Jones and C. J. van Rijsbergen. Report on the need for and provision of
%an ‘ideal’ test collection. Technical report, University Computer Laboratory, Cam- bridge, 1975
The subject of pooling bias has been extensively studied in the information retrieval (IR) community starting with \citet{zobel1998reliable}, which examined the effects of pooling bias on the TREC AdHoc task, but concluded that pooling bias was not a significant problem. %  and \citep{voorhees1999overview}
% zobel also mentions a reinforcing effect, but the dynamics are perhaps more dependent on the IR evaluation setup.
However, when the topic was later revisited,
%\citet{buckley2004incomplete} found that pooling bias could significantly change system ranking, and 
\citet{buckley2007bias} identified that the reason for the small bias was because the submissions to the task were too similar; upon repeating the experiment using a novel system as part of the TREC Robust track, they identified a 23\% point drop in average precision scores!%
\footnote{For the interested reader, \citet{webber2010measurement} presents an excellent survey of the literature on pooling bias.}
%We also use the methodology of \citet{zobel1998reliable} to measure pooling bias in KBP, but find that the pooling bias more significant of a problem in KBP evaluation than it has been in IR evaluation.
%%However, unlike in the information retrieval setting, we find a very strong effect \pl{what does strong effect mean?}.
%One explanation for this difference is that unlike evaluation metrics for KBP, popular IR metrics are rank-weighted, and unassessed documents, typically be lower in the ranking, contribute less to the evaluation score.

% - solutions to pooling bias 
Many solutions to the pooling bias problem have been proposed in the context of information retrieval, e.g.\ 
%  changing the queries to be more specific \citep{buckley2007bias}, 
  adaptively constructing the pool to collect relevant data more cost-effectively \citep{zobel1998reliable,cormack1998efficient,aslam2006statistical}, or
  modifying the scoring metrics to be less sensitive to unassessed data \citep{buckley2004incomplete,sakai2008information,aslam2006statistical}.
Many of these ideas exploit the ranking of documents in IR which does not apply to KBP.\@
%Furthermore, the pooling bias persists in TAC KBP evaluations even if \textit{all answers} reported by systems for a given set of entities are assessed for correctness.
While both \citet{aslam2006statistical} and \citet{yilmaz2008simple} estimate evaluation metrics by using importance sampling estimators, the techniques they propose require knowing the set of all submissions beforehand.
In contrast, our on-demand methodology can produce unbiased evaluation scores for new development systems as well.

%While pooling bias hasn't been studied on KBP before, \cite{surdeanau} does study the effect of incomplete labels on distant supervision.
% - crowdsourcing kbp and nlp
There have been several approaches taken to crowdsource data pertinent to knowledge base population \citep{vannella2014validating,angeli2014combining,he2015question,liu2016effective}.
The most extensive annotation effort is probably \citet{pavlick2016gun}, which crowdsources a knowledge base for gun-violence related events.
In contrast to previous work, our focus is on \textit{evaluating systems}, not collecting a dataset.
%As a result, we are able to integrate the systems we are evaluating while the data collection process.
Furthermore, our main contribution is not a large dataset, but an evaluation service that allows anyone to use crowdsourcing predictions made by their system. 

\section{Discussion}
\label{sec:kbpo:discussion}

Over the last ten years of the TAC KBP task, the gap between human and system performance has barely narrowed despite the community's best efforts: top automated systems score less than 36\% \fone{} while human annotators score more than 60\%.
% Cite Weber -- IR has much still to progress, hidden by variability in scores, etc.
% Prevalence of patterns based systems.
In this chapter, we've shown that the current evaluation methodology may be a contributing factor because of its bias against novel system improvements.
The new on-demand framework proposed in this chapter addresses this problem by obtaining human assessments of new system output through crowdsourcing.
The framework is made economically feasible by carefully sampling output to be assessed and correcting for sample bias through importance sampling.

% Presence of this platform allows us to decouple dataset creation from task. -- e.g. can use and evaluate distantly supervised datasets more easily.
Of course, simply providing better evaluation scores is only part of the solution and it is clear that better datasets are also necessary.
However, the very same difficulties in scale that make evaluating KBP difficult also make it hard to collect a high quality dataset for the task.
As a result, existing datasets \citep{angeli2014combining,adel2016comparing} have relied on the output of existing systems, making it likely that they exhibit the same biases against novel systems that we've discussed in this chapter.
We believe that providing a fair and standardized evaluation platform as a service
allows researchers to exploit such datasets and while still being able to accurately measure their performance on the knowledge base population task.

While our observations regarding bias were specifically instantiated and empirically measured for knowledge base population, they apply to any scenario exhibiting finite incompleteness, e.g.\ information retrieval, event extraction or extractive question answering (different from the open-response question answering we discussed in \refchap{intro}).
Our solution applies wherever precision or recall are being measured across multiple systems and can be easily extended to estimate any other metric that is averaged over instances in the data.
However, the extent to which we are able to reduce the variance of estimation, and thereby cost, depends on the overlap between the predictions of two systems: in practice, we expect the overlap to be fairly high in scenarios exhibiting finite incompleteness.
Put together, in this chapter we have shown that we can indeed reduce the costs of unbiased evaluation in finite incompleteness settings with importance-reweighted estimators.
