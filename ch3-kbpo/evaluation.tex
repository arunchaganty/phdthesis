\begin{figure}[!h]
  \centering

  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/simulation/simulation-p}
    \caption{}
  \end{subfigure}

  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/simulation/simulation-r}
    \caption{}
  \end{subfigure}

  \caption[Reduction of variance with the importance-reweighted estimator]{\label{fig:kbpo:simulation}
  A comparison of bias for the pooling, simple and joint estimators on the TAC KBP 2015 challenge.
  Each point in the figure is a mean of 500 repeated trials; dotted lines show the 90\% quartile.
  %The pooling based method uses between 5,000 and 6,000 labeled instances, while the sampling based methods use 
  %approximately 150 samples from each system.
  %Dashed trend lines indicate the mean bias of the estimation method: 
  %Unbiased estimates lie on the $y = x$ line.
  Both the simple and joint estimators are unbiased, and the joint estimator is able to significantly reduce variance.
  }
\end{figure}


\begin{figure}[!h]
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/simulation/simulation-n}
    \caption{}
  \end{subfigure}

  \begin{subfigure}{0.49\textwidth}
    \centering
    \input{evaluation.table}
    %\includegraphics[width=\columnwidth]{figures/kbp2016/kbp2016_f1}
    \vfill
    \caption{\label{fig:kbpo:evaluation-results}}
  \end{subfigure}

  \caption[An evaluation of the importance-reweighted estimator]{\label{fig:kbpo:evaluation}
  \textbf{(a):} 
  A comparison of the number of samples used to estimate scores under the fixed and adaptive sample selection scheme.
  %In the simulation, the top 40 systems were evaluated in randomized order to achieve a target variance equal to that obtained with 500 samples for a single system.
  Each faint line shows the number of samples used during a single trial, while solid lines show the mean over 100 trials.
  The dashed line shows a square-root relationship between the number of systems evaluated and the number of samples required.
  %\ac{Note: fitting with a cubic gives $x = 0.01 y^3 -0.1y^2 + 1.8 y - 1.1$ with $R=1.0$.}
  Thus joint estimation combined with adaptive sample selection can reduce the number of labeled annotations required by an order of magnitude.
  \textbf{(b):} 
Precision ($P$), recall ($R$) and \fone{} scores from a pilot run of our evaluation service for ensembles of a rule-based system (R), a logistic classifier (L) and a neural network classifier (N) run on the TAC KBP 2016 document corpus. 
  }
\end{figure}

\section{Experiments}
\label{sec:kbpo:evaluation}

Let us now see how well on-demand evaluation works in practice.
We begin by empirically studying the bias and variance of the joint estimator proposed in \refsec{kbpo:method} and find it is able to correct for pooling bias while significantly reducing variance in comparison with the simple estimator.
We then demonstrate that on-demand evaluation can serve as a practical replacement for the TAC KBP evaluations by piloting a new evaluation service we have developed to evaluate three distinct systems on TAC KBP 2016 document corpus.
%We find that we are able to obtain results of  quality in a cost effective manner.

\subsection{Bias and variance of the on-demand evaluation}
Once again, we use the labeled system predictions from the TAC KBP 2015 evaluation and treat them as an exhaustively annotated dataset.
To evaluate the pooling methodology we construct an evaluation dataset using
instances found by human annotators and labeled instances pooled from 9
randomly chosen teams (i.e.\ half the total number of participating teams), and
use this dataset to evaluate the remaining 9 teams.
On average, the pooled evaluation dataset contains between 5,000 and 6,000 labeled instances and evaluates 34 different systems (since each team may have submitted multiple systems).
Next, we evaluated sets of 9 randomly chosen teams with our proposed simple and joint estimators using a total of 5,000 samples:
about 150 of these samples are drawn from $\sY$, i.e.\ the full TAC KBP 2015 evaluation data, and 150 samples from each of the systems being evaluated.

We repeat the above simulated experiment 500 times and compare the estimated precision and recall with their true values (\reffig{kbpo:simulation}).
The simulations once again highlights that the pooled methodology is biased, while the simple and joint estimators are not.
Furthermore, the joint estimators significantly reduce variance relative to the simple estimators:
the median 90\% confidence intervals reduce from 0.14 to 0.06 precision and from 0.14 to 0.08 for recall.

\subsection{Number of samples required by on-demand evaluation}
Separately, we evaluate the efficacy of the adaptive sample selection method described in \refsec{kbpo:joint} through another simulated experiment.
In each trial of this experiment, we evaluate the top 40 systems in random order.
As each subsequent system is evaluated, the number of samples to pick from the system is chosen to meet a target variance and added to the current pool of labeled instances.
To make the experiment more interpretable, we choose the target variance to correspond with the estimated variance of having 500 samples.
\reffig{kbpo:evaluation} plots the results of the experiment.
The number of samples required to estimate systems quickly drops off from the benchmark of 500 samples as the pool of labeled instances covers more systems.
This experiment shows that on-demand evaluation using joint estimation can scale up to an order of magnitude more submissions  than a simple estimator for the same cost.

\subsection{A mock evaluation for TAC KBP 2016}
We have implemented the on-demand evaluation framework described here as an evaluation service to which researchers can submit their own system predictions.
As a pilot of the service, we evaluated three relation extraction systems that also participated in the official 2016 TAC KBP competition.
Each system uses Stanford CoreNLP~\citep{manning2014stanford} to identify entities, the Illinois Wikifier~\citep{ratinov2011local} to perform entity linking and a combination of a rule-based system (P), a logistic classifier (L), and a neural network classifier (N) for relation extraction.
%distinct relation extraction systems (a rule-based system, a logistic classifier, and a neural network classifier) on 15,000 Newswire documents from 2016 TAC KBP evaluation.
We used 15,000 Newswire documents from the 2016 TAC KBP evaluation as our document corpus.
In total, 100 documents were exhaustively annotated for about \$2,000 and 500 instances from each system were labeled for about \$150 each.
Evaluating all three system only took about 2 hours. 

%In total, 100 documents were exhaustively annotated for about \$2,000, and 1,000 instances from each system were labeled for about \$300 each, with 500 sampled to estimate macro-averaged relation scores and 500 sampled to estimate macro-averaged entity scores.
\reffig{kbpo:evaluation-results} reports scores obtained through on-demand evaluation of these systems as well as their corresponding official TAC evaluation scores.
While the relative ordering of systems between the two evaluations is the same, we note that precision and recall as measured through on-demand evaluation are respectively higher and lower than the official scores.
This is to be expected because on-demand evaluation measures precision using each systems output as opposed to an externally defined set of evaluation entities.
Likewise, recall is measured using exhaustive annotations of relations within the corpus instead of annotations from pooled output in the official evaluation.  
