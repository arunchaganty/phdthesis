\section{Game playing}
\label{sec:otj:game-playing}

In \refsec{otj:model} we modeled on-the-job learning as a stochastic game played between the system and the crowd.
%and defined a model of the environment and utility for the system to maximize.
We now turn to the problem of actually finding a policy that maximizes the expected utility,
which is, of course, intractable because of the large state space.

Our algorithm (\refalg{otj:mcvts}) combines ideas from Monte Carlo tree search~\citep{kocsis2006bandit} to systematically explore the state space and 
progressive widening~\citep{coulom2007computing} to deal with the challenge of continuous variables (time).
Some intuition about the algorithm is provided below.
When simulating the system's turn, the next state (and hence action) is chosen using the upper confidence tree (UCT) decision rule that trades off maximizing the value of the next state (exploitation) with the number of visits (exploration).
The crowd's turn is simulated based on transitions defined in \refsec{otj:model}.
To handle the unbounded fanout during the crowd's turn, we use progressive widening  
that maintains a current set of ``active'' or ``explored'' states, which is gradually grown with time. 
Let $N(\sigma)$ be the number of times a state has been visited,
and $C(\sigma)$ be all successor states that the algorithm has sampled.


\begin{algorithm}
%  \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%  \renewcommand{\algorithmicensure}{\textbf{Output:}}
\caption{Approximating expected utility with MCTS and progressive widening}
\label{alg:otj:mcvts}
  \begin{algorithmic}[1]
    \State For all $\sigma$, $N(\sigma) \gets 0$, $V(\sigma) \gets 0$, $C(\sigma) \gets [\,]$
	\Comment Initialize visits, utility sum, and children
    \Function{monteCarloValue}{state $\sigma$}
    \State increment $N(\sigma)$
    \If{\text{system's turn}}
    \State $\sigma' \gets \argmax_{\sigma'} \left\{\frac{V(\sigma')}{N(\sigma')} + c \sqrt{\frac{\log N(\sigma)}{N(\sigma')}} \right\}$
      \Comment Choose next state $\sigma'$ using UCT
	  \State $v \gets $\Call{monteCarloValue}{$\sigma'$}
      \State $V(\sigma) \gets V(\sigma) + v$
	  \Comment Record observed utility
      %\State $n.\scvisits \gets n.\scvisits + 1, n'.\scvisits \gets n'.\scvisits + 1$
      \State \Return $v$
    \ElsIf{crowd's turn}
	  \If{$\max(1,\sqrt{N(\sigma)}) \leq |C(\sigma)|$}
	  \Comment Restrict continuous samples using PW
        \State $\sigma'$ is sampled from set of already visited $C(\sigma)$ based on (\ref{eqn:otj:dynamics})
	  \Else
        \State $\sigma'$ is drawn based on (\ref{eqn:otj:dynamics})
        \State $C(\sigma) \gets C(\sigma) \cup \{ [\sigma'] \}$
	  \EndIf
      \State \Return \Call{monteCarloValue}{$\sigma'$}
    \ElsIf{game terminated}
    \State \Return utility $U$ of $\sigma$ according to (\ref{eqn:otj:utility})
    \EndIf
    \EndFunction{}
    %\Function{$\scvalue^f$}{node $n$}
      %\State \Return $\theta^\top \phi(n)$
      %\Comment Linear function approximation
    %\EndFunction{}
  \end{algorithmic}
\end{algorithm}

