\section{Conclusion}
\label{sec:otj:conclusion}

We have introduced a new framework that learns from (noisy) crowds \emph{on-the-job}
to maintain high accuracy, and reducing cost significantly over time.
The technical core of our approach is modeling the on-the-job setting
as a stochastic game and using ideas from game playing to approximate the optimal policy.
%We addressed the problem of deciding what supervision to ask the crowd for by
%modeling the problem as a stochastic game played by the model and the crowd. 
%Using Monte Carlo tree search to approximate the optimal policy,
We have built a system, LENSE, %, that is able to trade off time and money for accuracy,
which obtains significant cost reductions over a pure crowd approach
and significant accuracy improvements over a pure ML approach.
%We have shown that learning on-the-job can achieve comparable accuracies to
%asking humans without. 

% PL: not needed
%An increasing number of problems that challenge AI today involve interactive tasks that are not suitable to conventional dataset collection, for example, dialogue or question answering.
%We envision on-the-job training as enabling these tasks.
%Of course, there are many problems for which it is hard to ask turkers to give labels. Future work: ask for partial supervision through the measurements framework.
More broadly, in this chapter we have shown how the uncertainty estimates of a statistical model can be used to identify and resolve \textit{incompleteness in the training data} at \textit{test time}.
% not unbiased, but still a form of evaluation.
In contrast to the methods in \refchap{kbpo} and \refchap{price}, the approach we propose in this chapter is not unbiased, but rather integrates human feedback into a statistical model and uses its own uncertainty to evaluate the performance of the model.
As a result, the model is robust to annotation errors, which posed a problem in \refchap{price}, that strongly contradict the models own prior belief by requesting for more annotations.
On the contrary, the method relies on the model being well-calibrated and its estimated accuracy can be significantly wrong if this condition is not met.
% necessary for evaluating interactive tasks.
In addition to being pragmatic, requesting humans to correct model predictions is also important when evaluating multi-turn interactive systems such as dialogue systems:
  an error early on the conversation limits our ability to evaluate performance later in the conversation. 
By using on-demand human feedback, we can correct such errors before they arise and thus increase the scope of evaluation.
