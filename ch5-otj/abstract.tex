Having seen the power of introducing human annotations to properly evaluate systems in the last two chapters, we now explore ways in which we can use human evaluation to make systems better.
In this chapter, our goal is to deploy a high-accuracy system starting with zero training examples.
We consider an ``on-the-job'' setting, where as inputs arrive, we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries
decreases. We cast our setting as a stochastic game based on Bayesian decision
theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approx-
imation based on Monte Carlo Tree Search. We tested our approach on three
datasets---named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full
human annotation, while boosting performance relative to the expert provided labels. We also achieve a $8\%$ \fone{} improvement over having a single
human label the whole set, and a $28\%$ \fone{} improvement over online learning.
