\section{Introduction}
\label{sec:intro}

% Setting: want to deploy system without training examples
% Existing solutions: pure ML, pure crowd
There are two roads to an accurate AI system today:
(i) gather a huge amount of labeled training data \citep{deng2009imagenet} and do supervised learning \citep{krizhevsky2012imagenet};
or (ii) use crowdsourcing to directly perform the task \citep{bernstein2010soylent,kokkalis2013emailvalet}.
However, both solutions require non-trivial amounts of time and money.
In many situations, one wishes to build a new system --- e.g., to do Twitter information extraction
\citep{li2012twiner} to aid in disaster relief efforts or monitor public
opinion --- but one simply lacks the resources to follow either the pure ML or pure crowdsourcing road.

% Our proposal: on the job training setting, related work
In this chapter, we propose a framework called \emph{on-the-job learning} (formalizing and extending ideas first implemented in \citep{lasecki2013realtime}),
in which we produce high quality results from the start without requiring a trained model.
When a new input arrives,
the system can choose to asynchronously query the crowd on \emph{parts} of the input it is
uncertain about (e.g.\ query about the label of a single token in a sentence). After collecting enough evidence the system makes a prediction.
The goal is to maintain high accuracy by initially using the crowd as a crutch,
but gradually becoming more self-sufficient as the model improves.
Online learning \citep{cesabianchi06prediction} and
online active learning \citep{helmbold1997some,sculley2007online,chu2011unbiased}
are different in that
they do not actively seek new information \emph{prior} to making a prediction,
and cannot maintain high accuracy independent of the number of data instances seen so far.
Active classification \citep{gao2011active}, like us,
strategically seeks information (by querying a subset of labels) prior to prediction,
%also queries a subset of the labels strategically,
but it is based on a static policy, 
whereas we improve the model during test time based on observed data.

% Bayesian decision theory: technical solution
To determine which queries to make,
we model on-the-job learning as a stochastic game based on a CRF prediction model.
We use Bayesian decision theory to tradeoff latency, cost, and accuracy in a principled manner.
Our framework naturally gives rise to intuitive strategies:
To achieve high accuracy, we should ask for redundant labels
to offset the noisy responses.  To achieve low latency, we should issue queries
in parallel, whereas if latency is unimportant, we should issue queries
sequentially in order to be more adaptive.
Computing the optimal policy is intractable,
so we develop an approximation
based on Monte Carlo tree search \citep{kocsis2006bandit} and
progressive widening to reason about continuous time \citep{coulom2007computing}.

% Experiments.
We implemented and evaluated our system on three different tasks: named-entity
recognition, sentiment classification, and image classification.
On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8\% F1 improvement over having a single human label the whole set, and a 28\% F1 improvement over online learning.
An open-source implementation of our system, dubbed LENSE for ``Learning from Expensive Noisy Slow Experts'' is available at
\href{http://www.github.com/keenon/lense}{http://www.github.com/keenon/lense}.
