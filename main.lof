\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Overview of some information summarization tasks}}{2}{figure.caption.6}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Complete and incomplete evaluation sets}}{3}{figure.caption.7}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Examples highlighting the limitations of incomplete evaluation sets}}{5}{figure.caption.8}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces An example describing entities and relations in knowledge base population.\relax }}{11}{figure.caption.12}
\contentsline {figure}{\numberline {3.2}{\ignorespaces In pooled evaluation, an evaluation dataset is constructed by labeling relation instances collected from the pooled systems (A and B) and from a team of human annotators (Humans). However, when a new system (C) is evaluated on this dataset, some of its predictions ($i_6$) are missing and can not be fairly evaluated. Here, the precision and recall for C should be $\frac {3}{3}$ and $\frac {3}{4}$ respectively, but its evaluation scores are estimated to be $\frac {2}{3}$ and $\frac {2}{3}$. The discrepancy between these two scores is called \textit {pooling bias}. \relax }}{13}{figure.caption.13}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Median pooling bias (difference between pooled and unpooled scores) on the top 40 systems of TAC KBP 2015 evaluation using the official and \texttt {anydoc} {} scores. The bias is much smaller for the lenient \texttt {anydoc} {} metric, but even so, it is larger than the largest difference between adjacent systems (1.5\% \ensuremath {F_1}{}) and typical system improvements (around 1\% \ensuremath {F_1}{}). \relax }}{27}{figure.caption.15}
\contentsline {figure}{\numberline {3.4}{\ignorespaces \textbf {(a, b):} Interfaces for annotating relations and entities respectively. \textbf {(c, d):} A comparison of bias for the pooling, simple and joint estimators on the TAC KBP 2015 challenge. Each point in the figure is a mean of 500 repeated trials; dotted lines show the 90\% quartile. Both the simple and joint estimators are unbiased, and the joint estimator is able to significantly reduce variance. \textbf {(e):} A comparison of the number of samples used to estimate scores under the fixed and adaptive sample selection scheme. Each faint line shows the number of samples used during a single trial, while solid lines show the mean over 100 trials. The dashed line shows a square-root relationship between the number of systems evaluated and the number of samples required. Thus joint estimation combined with adaptive sample selection can reduce the number of labeled annotations required by an order of magnitude. \textbf {(f):} Precision ($P$), recall ($R$) and \ensuremath {F_1}{} scores from a pilot run of our evaluation service for ensembles of a rule-based system (R), a logistic classifier (L) and a neural network classifier (N) run on the TAC KBP 2016 document corpus. \relax }}{28}{figure.caption.21}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces (a) At a system-level, automatic metrics (ROUGE-L) and human judgment correlate well, but (b) the instance-level correlation plot (where each point is a system prediction) shows that the instance-level correlation is quite low ($\rho = 0.31$). As a consequence, if we try to locally improve systems to produce better answers ($\triangleright $ in (a)), they do not significantly improve ROUGE scores and vice versa ($\vartriangle $). \relax }}{1}{figure.caption.22}
\contentsline {figure}{\numberline {4.2}{\ignorespaces The samples from $f(z)$ have a higher variance than the samples from $f(z)-g(z)$ but the same mean. This is the key idea behind using control variates to reduce variance.\relax }}{7}{figure.caption.24}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Inverse data efficiency for various values of $\gamma $ and $\rho $. We need both low $\gamma $ and high $\rho $ to obtain significant gains. \relax }}{9}{figure.caption.25}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Screenshots of the annotation interfaces we used to measure (a) summary language quality on CNN/Daily Mail and (b) answer correctness on MS MARCO tasks. \relax }}{11}{figure.caption.26}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Correlations of different automatic metrics on the MS MARCO and CNN/Daily Mail tasks. Certain systems are more correlated with certain automatic metrics than others, but overall the correlation is low to moderate for most systems and metrics. \relax }}{14}{figure.caption.30}
\contentsline {figure}{\numberline {4.6}{\ignorespaces 80\% bootstrap confidence interval length as a function of the number of human judgments used when evaluating the indicated systems on their respective datasets and prompts. (a) We see a modest reduction in variance (and hence cost) relative to human evaluation by using the VecSim automatic metric with the proposed control variates estimator to estimate \texttt {Overall} scores on the CNN/Daily Mail task; the data efficiency (DE) is $1.06$. (b) By improving the evaluation prompt to use \texttt {Edit}s instead, it is possible to further reduce variance relative to humans (DE is $1.15$). (c) Another way to reduce variance relative to humans is to improve the automatic metric evaluation; here using ROUGE-1 instead of VecSim improves the DE from $1.03$ to $1.16$. \relax }}{15}{figure.caption.32}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Named entity recognition on tweets in on-the-job learning. \relax }}{20}{figure.caption.34}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Example behavior while running structure prediction on the tweet ``Soup on George str.'' We omit the \textsc {resource}{} from the game tree for visual clarity. \relax }}{22}{figure.caption.35}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparing \ensuremath {F_1}{} and queries per token on the NER task over time. The left graph compares LENSE to online learning (which cannot query humans at test time). This highlights that LENSE maintains high \ensuremath {F_1}{} scores even with very small training set sizes, by falling back the crowd when it is unsure. The right graph compares query rate over time to 1-vote. This clearly shows that as the model learns, it needs to query the crowd less.\relax }}{29}{figure.caption.46}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Queries per example for LENSE on Sentiment. With simple \textsc {unigram} features, the model quickly learns it does not have the capacity to answer confidently and must query the crowd. With more complex \textsc {rnn} features, the model learns to be more confident and queries the crowd less over time.\relax }}{30}{table.caption.44}
\addvspace {10\p@ }
